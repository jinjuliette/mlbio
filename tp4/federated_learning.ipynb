{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "juliette.jin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10, MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset (or any other dataset like HAM 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='data/', download=True, transform=transforms.ToTensor())\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "train_loader = DataLoader(train_ds, batch_size=128)\n",
    "val_loader = DataLoader(val_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract two subsets of 600 data points each (without intersection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset1_indices = torch.randperm(50000)[:600]\n",
    "subset2_indices = torch.randperm(50000)[:600]\n",
    "subset1 = torch.utils.data.Subset(train_ds, subset1_indices)\n",
    "subset2 = torch.utils.data.Subset(train_ds, subset2_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple Convolutional Neural Network (2 convolutional layers and 2 dense layers, for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(512, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1)  # Adjusted shape to match input size\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function average_model_parameters(models: iterable, average_weight): iterable that takes a list of models as an argument and returns the weighted average of the parameters of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_model_parameters(models, average_weight):\n",
    "    new_model = Net()\n",
    "    # For each parameter in the new model, average the corresponding parameters in the input models\n",
    "    for new_param, *params in zip(new_model.parameters(), *[model.parameters() for model in models]):\n",
    "        new_param.data = sum(param.data for param in params) * average_weight\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that updates the parameters of a model from a list of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_parameters(model, parameters):\n",
    "    for param, new_param in zip(model.parameters(), parameters):\n",
    "        param.data = new_param.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a script/code/function that reproduces Algorithm 1, considering that both models are on your machine. Use an average_weight=[1/2, 1/2]. Reuse the same setup as in the article (50 examples per local batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm1(model1, model2, X_train1, y_train1, X_train2, y_train2, average_weight):\n",
    "    # Initialize the models with the same parameters\n",
    "    update_model_parameters(model1, model2.parameters())\n",
    "\n",
    "    # Split the training data into local batches\n",
    "    local_batch_size = 50\n",
    "    X_train1_batches = X_train1.split(local_batch_size)\n",
    "    y_train1_batches = y_train1.split(local_batch_size)\n",
    "    X_train2_batches = X_train2.split(local_batch_size)\n",
    "    y_train2_batches = y_train2.split(local_batch_size)\n",
    "\n",
    "    # Train the models on the local data\n",
    "    for X1, y1, X2, y2 in zip(X_train1_batches, y_train1_batches, X_train2_batches, y_train2_batches):\n",
    "        # Train model 1 on the local data\n",
    "        optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01)\n",
    "        model1.train()\n",
    "        optimizer1.zero_grad()\n",
    "        output1 = model1(X1)\n",
    "        loss1 = F.nll_loss(output1, y1)\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        # Train model 2 on the local data\n",
    "        optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.01)\n",
    "        model2.train()\n",
    "        optimizer2.zero_grad()\n",
    "        output2 = model2(X2)\n",
    "        loss2 = F.nll_loss(output2, y2)\n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "   \n",
    "    # Average the parameters of the models\n",
    "    new_model = average_model_parameters([model1, model2], average_weight)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your models without initializing the common parameters and measure the performance on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Model 1: Test Loss: 1.824346145248413, Accuracy: 62.83, Model 2: Test Loss: 1.8087377834320069, Accuracy: 54.92\n",
      "Epoch: 2/20, Model 1: Test Loss: 0.6467162405967712, Accuracy: 82.0, Model 2: Test Loss: 0.6595020809173584, Accuracy: 80.24\n",
      "Epoch: 3/20, Model 1: Test Loss: 0.433419891166687, Accuracy: 88.38, Model 2: Test Loss: 0.4738588337421417, Accuracy: 86.25\n",
      "Epoch: 4/20, Model 1: Test Loss: 0.36560064415931703, Accuracy: 89.83, Model 2: Test Loss: 0.4000449149131775, Accuracy: 88.31\n",
      "Epoch: 5/20, Model 1: Test Loss: 0.30979505019187925, Accuracy: 91.19, Model 2: Test Loss: 0.34133145456314085, Accuracy: 90.13\n",
      "Epoch: 6/20, Model 1: Test Loss: 0.27706118597984314, Accuracy: 92.29, Model 2: Test Loss: 0.310067565536499, Accuracy: 90.65\n",
      "Epoch: 7/20, Model 1: Test Loss: 0.2525043376922607, Accuracy: 92.83, Model 2: Test Loss: 0.2768687617182732, Accuracy: 91.78\n",
      "Epoch: 8/20, Model 1: Test Loss: 0.22703383046388625, Accuracy: 93.55, Model 2: Test Loss: 0.2491548327922821, Accuracy: 92.57\n",
      "Epoch: 9/20, Model 1: Test Loss: 0.2107682928085327, Accuracy: 93.92, Model 2: Test Loss: 0.22777490974664688, Accuracy: 93.29\n",
      "Epoch: 10/20, Model 1: Test Loss: 0.19549013921022415, Accuracy: 94.43, Model 2: Test Loss: 0.21408311643600464, Accuracy: 93.64\n",
      "Epoch: 11/20, Model 1: Test Loss: 0.18029018844366074, Accuracy: 94.85, Model 2: Test Loss: 0.19906551197767258, Accuracy: 94.06\n",
      "Epoch: 12/20, Model 1: Test Loss: 0.17009469230175017, Accuracy: 95.04, Model 2: Test Loss: 0.18492091252803802, Accuracy: 94.51\n",
      "Epoch: 13/20, Model 1: Test Loss: 0.15957855680584906, Accuracy: 95.38, Model 2: Test Loss: 0.17401246722340583, Accuracy: 94.93\n",
      "Epoch: 14/20, Model 1: Test Loss: 0.15388125849962234, Accuracy: 95.45, Model 2: Test Loss: 0.16546956524848938, Accuracy: 95.06\n",
      "Epoch: 15/20, Model 1: Test Loss: 0.14569745776057244, Accuracy: 95.76, Model 2: Test Loss: 0.15827623770833016, Accuracy: 95.3\n",
      "Epoch: 16/20, Model 1: Test Loss: 0.13920876155495643, Accuracy: 95.84, Model 2: Test Loss: 0.15237816682159902, Accuracy: 95.54\n",
      "Epoch: 17/20, Model 1: Test Loss: 0.13396777125298978, Accuracy: 96.02, Model 2: Test Loss: 0.14499114687740802, Accuracy: 95.81\n",
      "Epoch: 18/20, Model 1: Test Loss: 0.12899200137853623, Accuracy: 96.16, Model 2: Test Loss: 0.13879533311128617, Accuracy: 95.76\n",
      "Epoch: 19/20, Model 1: Test Loss: 0.12646779127418994, Accuracy: 96.11, Model 2: Test Loss: 0.13258245763778687, Accuracy: 96.03\n",
      "Epoch: 20/20, Model 1: Test Loss: 0.12104813427329063, Accuracy: 96.31, Model 2: Test Loss: 0.1299968879967928, Accuracy: 96.01\n"
     ]
    }
   ],
   "source": [
    "model1 = Net()\n",
    "model2 = Net()\n",
    "\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.01)\n",
    "\n",
    "def train(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model1, optimizer1, train_loader)\n",
    "    train(model2, optimizer2, train_loader)\n",
    "    test_loss1, accuracy1 = test(model1, val_loader)\n",
    "    test_loss2, accuracy2 = test(model2, val_loader)\n",
    "    print(f'Epoch: {epoch+1}/{epochs}, Model 1: Test Loss: {test_loss1}, Accuracy: {accuracy1}, Model 2: Test Loss: {test_loss2}, Accuracy: {accuracy2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your models with the initialization of common parameters and verify that the performance is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model: Test Loss: 0.13597940123975277, Accuracy: 95.8\n"
     ]
    }
   ],
   "source": [
    "X_train1 = torch.cat([data for data, _ in DataLoader(subset1, batch_size=600)])\n",
    "X_train2 = torch.cat([data for data, _ in DataLoader(subset2, batch_size=600)])\n",
    "y_train1 = torch.cat([target for _, target in DataLoader(subset1, batch_size=600)])\n",
    "y_train2 = torch.cat([target for _, target in DataLoader(subset2, batch_size=600)])\n",
    "\n",
    "average_weight = torch.tensor([1/2, 1/2])\n",
    "\n",
    "# Fix the average_model_parameters function to correctly average the parameters\n",
    "def average_model_parameters(models, average_weight):\n",
    "    new_model = Net()\n",
    "    for new_param, params in zip(new_model.parameters(), zip(*[model.parameters() for model in models])):\n",
    "        new_param.data = sum(w * param.data for w, param in zip(average_weight, params))\n",
    "    return new_model\n",
    "\n",
    "new_model = algorithm1(model1, model2, X_train1, y_train1, X_train2, y_train2, average_weight)\n",
    "test_loss, accuracy = test(new_model, val_loader)\n",
    "print(f'New Model: Test Loss: {test_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the number of data points in each sub-batch. What is the minimum number of data points necessary for the final model to have acceptable performance? Repeat the study on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 256, New Model: Test Loss: 0.132783861297369, Accuracy: 96.0\n",
      "Batch Size: 128, New Model: Test Loss: 0.13173209536969663, Accuracy: 95.88\n",
      "Batch Size: 64, New Model: Test Loss: 0.13790862982869148, Accuracy: 95.77\n",
      "Batch Size: 32, New Model: Test Loss: 0.1397119868248701, Accuracy: 95.76\n",
      "Batch Size: 16, New Model: Test Loss: 0.1367542265355587, Accuracy: 95.99\n",
      "Batch Size: 8, New Model: Test Loss: 0.13529405716359616, Accuracy: 95.96\n",
      "Batch Size: 4, New Model: Test Loss: 0.1372001058280468, Accuracy: 95.84\n",
      "Batch Size: 2, New Model: Test Loss: 0.2897350154221058, Accuracy: 90.33\n",
      "Batch Size: 1, New Model: Test Loss: 0.28947960694432257, Accuracy: 90.34\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [256, 128, 64, 32, 16, 8, 4, 2, 1]:\n",
    "    subset1_indices = torch.randperm(50000)[:batch_size]\n",
    "    subset2_indices = torch.randperm(50000)[:batch_size]\n",
    "    subset1 = torch.utils.data.Subset(train_ds, subset1_indices)\n",
    "    subset2 = torch.utils.data.Subset(train_ds, subset2_indices)\n",
    "    X_train1 = torch.cat([data for data, _ in DataLoader(subset1, batch_size=batch_size)])\n",
    "    X_train2 = torch.cat([data for data, _ in DataLoader(subset2, batch_size=batch_size)])\n",
    "    y_train1 = torch.cat([target for _, target in DataLoader(subset1, batch_size=batch_size)])\n",
    "    y_train2 = torch.cat([target for _, target in DataLoader(subset2, batch_size=batch_size)])\n",
    "    new_model = algorithm1(model1, model2, X_train1, y_train1, X_train2, y_train2, average_weight)\n",
    "    test_loss, accuracy = test(new_model, val_loader)\n",
    "    print(f'Batch Size: {batch_size}, New Model: Test Loss: {test_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the minimum number of data points necessary for the final model to have acceptable performance?**\n",
    "\n",
    "A baseline accuracy would be around 92% or 93% for MNIST.\n",
    "\n",
    "But as we are using a CNN we should aim for a higher accuracy, more around 97% or more, which we are closely getting to the state of the art.\n",
    "\n",
    "Maybe 96% would be a good threshold. And if we had done a bit more of training we could have reached 97% or more.\n",
    "\n",
    "So I'd go with at least 256 here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cf_dataset = CIFAR10(root='data/', download=True, transform=transforms.ToTensor())\n",
    "cf_train_ds, cf_val_ds = random_split(cf_dataset, [45000, 5000])\n",
    "cf_train_loader = DataLoader(cf_train_ds, batch_size=128)\n",
    "cf_val_loader = DataLoader(cf_val_ds, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_subset1_indices = torch.randperm(len(cf_train_ds))[:600]\n",
    "cf_subset2_indices = torch.randperm(len(cf_train_ds))[:600]\n",
    "cf_subset1 = torch.utils.data.Subset(cf_train_ds, cf_subset1_indices)\n",
    "cf_subset2 = torch.utils.data.Subset(cf_train_ds, cf_subset2_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CFNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "cf_model = CFNet()\n",
    "cf_model1 = CFNet()\n",
    "cf_model2 = CFNet()\n",
    "\n",
    "cf_optimizer1 = optim.SGD(cf_model1.parameters(), lr=0.01)\n",
    "cf_optimizer2 = optim.SGD(cf_model2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Model 1: Test Loss: 2.263892604446411, Accuracy: 15.06, Model 2: Test Loss: 2.269119260787964, Accuracy: 15.82\n",
      "Epoch: 2/20, Model 1: Test Loss: 2.225563650894165, Accuracy: 15.68, Model 2: Test Loss: 2.2297130237579346, Accuracy: 14.46\n",
      "Epoch: 3/20, Model 1: Test Loss: 2.202763724517822, Accuracy: 16.58, Model 2: Test Loss: 2.203890519332886, Accuracy: 15.52\n",
      "Epoch: 4/20, Model 1: Test Loss: 2.1784815063476564, Accuracy: 18.46, Model 2: Test Loss: 2.16588244972229, Accuracy: 15.8\n",
      "Epoch: 5/20, Model 1: Test Loss: 2.15202410697937, Accuracy: 18.94, Model 2: Test Loss: 2.1069220703125, Accuracy: 17.44\n",
      "Epoch: 6/20, Model 1: Test Loss: 2.1064886562347414, Accuracy: 23.04, Model 2: Test Loss: 2.059283430480957, Accuracy: 23.76\n",
      "Epoch: 7/20, Model 1: Test Loss: 2.047286938095093, Accuracy: 25.36, Model 2: Test Loss: 2.021228925704956, Accuracy: 24.52\n",
      "Epoch: 8/20, Model 1: Test Loss: 1.9901579093933106, Accuracy: 28.14, Model 2: Test Loss: 2.0120998863220216, Accuracy: 26.92\n",
      "Epoch: 9/20, Model 1: Test Loss: 1.9516969963073731, Accuracy: 30.06, Model 2: Test Loss: 1.9875987972259521, Accuracy: 25.7\n",
      "Epoch: 10/20, Model 1: Test Loss: 1.8978724235534667, Accuracy: 30.96, Model 2: Test Loss: 1.9487143852233886, Accuracy: 27.38\n",
      "Epoch: 11/20, Model 1: Test Loss: 1.8544475910186768, Accuracy: 33.92, Model 2: Test Loss: 1.9243116413116454, Accuracy: 27.82\n",
      "Epoch: 12/20, Model 1: Test Loss: 1.8085249168395996, Accuracy: 35.88, Model 2: Test Loss: 1.9055665363311767, Accuracy: 31.68\n",
      "Epoch: 13/20, Model 1: Test Loss: 1.7743553882598877, Accuracy: 37.02, Model 2: Test Loss: 1.8614891891479493, Accuracy: 33.58\n",
      "Epoch: 14/20, Model 1: Test Loss: 1.763209642791748, Accuracy: 37.94, Model 2: Test Loss: 1.8348283313751221, Accuracy: 34.88\n",
      "Epoch: 15/20, Model 1: Test Loss: 1.7041022094726563, Accuracy: 40.4, Model 2: Test Loss: 1.830979945755005, Accuracy: 36.12\n",
      "Epoch: 16/20, Model 1: Test Loss: 1.6896404554367066, Accuracy: 40.48, Model 2: Test Loss: 1.7816041187286378, Accuracy: 37.52\n",
      "Epoch: 17/20, Model 1: Test Loss: 1.662442733001709, Accuracy: 42.12, Model 2: Test Loss: 1.7767169410705566, Accuracy: 38.0\n",
      "Epoch: 18/20, Model 1: Test Loss: 1.6580621740341186, Accuracy: 41.68, Model 2: Test Loss: 1.736353973388672, Accuracy: 39.8\n",
      "Epoch: 19/20, Model 1: Test Loss: 1.6346852214813232, Accuracy: 42.78, Model 2: Test Loss: 1.7054947092056274, Accuracy: 40.54\n",
      "Epoch: 20/20, Model 1: Test Loss: 1.6190967977523805, Accuracy: 43.02, Model 2: Test Loss: 1.68082299118042, Accuracy: 41.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(cf_model1, cf_optimizer1, cf_train_loader)\n",
    "    train(cf_model2, cf_optimizer2, cf_train_loader)\n",
    "    cf_test_loss1, cf_accuracy1 = test(cf_model1, cf_val_loader)\n",
    "    cf_test_loss2, cf_accuracy2 = test(cf_model2, cf_val_loader)\n",
    "    print(f'Epoch: {epoch+1}/{epochs}, Model 1: Test Loss: {cf_test_loss1}, Accuracy: {cf_accuracy1}, Model 2: Test Loss: {cf_test_loss2}, Accuracy: {cf_accuracy2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Model: Test Loss: 1.6659834827423097, Accuracy: 40.06\n"
     ]
    }
   ],
   "source": [
    "cf_X_train1 = torch.cat([data for data, _ in DataLoader(cf_subset1, batch_size=len(cf_subset1))])\n",
    "cf_X_train2 = torch.cat([data for data, _ in DataLoader(cf_subset2, batch_size=len(cf_subset2))])\n",
    "cf_y_train1 = torch.cat([target for _, target in DataLoader(cf_subset1, batch_size=len(cf_subset1))])\n",
    "cf_y_train2 = torch.cat([target for _, target in DataLoader(cf_subset2, batch_size=len(cf_subset2))])\n",
    "\n",
    "cf_average_weight = torch.tensor([1/2, 1/2])\n",
    "\n",
    "cf_new_model = algorithm1(cf_model1, cf_model2, cf_X_train1, cf_y_train1, cf_X_train2, cf_y_train2, cf_average_weight)\n",
    "cf_test_loss, cf_accuracy = test(cf_new_model, cf_val_loader)\n",
    "print(f'New Model: Test Loss: {cf_test_loss}, Accuracy: {cf_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 256, New Model: Test Loss: 1.8403869047164918, Accuracy: 29.02\n",
      "Batch Size: 128, New Model: Test Loss: 1.6881108165740968, Accuracy: 39.82\n",
      "Batch Size: 64, New Model: Test Loss: 1.689157536506653, Accuracy: 38.9\n",
      "Batch Size: 32, New Model: Test Loss: 1.719276428604126, Accuracy: 36.38\n",
      "Batch Size: 16, New Model: Test Loss: 1.7397148279190064, Accuracy: 33.98\n",
      "Batch Size: 8, New Model: Test Loss: 1.7265831121444701, Accuracy: 34.28\n",
      "Batch Size: 4, New Model: Test Loss: 1.7187852586746215, Accuracy: 35.54\n",
      "Batch Size: 2, New Model: Test Loss: 2.701066907119751, Accuracy: 14.06\n",
      "Batch Size: 1, New Model: Test Loss: 2.4819874057769775, Accuracy: 18.96\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [256, 128, 64, 32, 16, 8, 4, 2, 1]:\n",
    "    cf_subset1_indices = torch.randperm(len(cf_train_ds))[:batch_size]\n",
    "    cf_subset2_indices = torch.randperm(len(cf_train_ds))[:batch_size]\n",
    "    cf_subset1 = torch.utils.data.Subset(cf_train_ds, cf_subset1_indices)\n",
    "    cf_subset2 = torch.utils.data.Subset(cf_train_ds, cf_subset2_indices)\n",
    "    cf_X_train1 = torch.cat([data for data, _ in DataLoader(cf_subset1, batch_size=len(cf_subset1))])\n",
    "    cf_X_train2 = torch.cat([data for data, _ in DataLoader(cf_subset2, batch_size=len(cf_subset2))])\n",
    "    cf_y_train1 = torch.cat([target for _, target in DataLoader(cf_subset1, batch_size=len(cf_subset1))])\n",
    "    cf_y_train2 = torch.cat([target for _, target in DataLoader(cf_subset2, batch_size=len(cf_subset2))])\n",
    "    cf_new_model = algorithm1(cf_model1, cf_model2, cf_X_train1, cf_y_train1, cf_X_train2, cf_y_train2, cf_average_weight)\n",
    "    cf_test_loss, cf_accuracy = test(cf_new_model, cf_val_loader)\n",
    "    print(f'Batch Size: {batch_size}, New Model: Test Loss: {cf_test_loss}, Accuracy: {cf_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we've repeated the study on CIFAR-10.\n",
    "\n",
    "The baseline accuracy would be around 10% or ~20 to 30% for the simpliest models (random, linear models, etc)\n",
    "\n",
    "For a basic CNN we should aim for a higher accuracy, more around 70% or more, which is not what we are getting here. We are in between a random model and a basic CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
