{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "juliette.jin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpI9eESpkREM"
   },
   "source": [
    "# Ex1: Onehot coding DNA\n",
    "\n",
    "Write a function called **onehot_dna(dna_str)** that allows to encode a DNA segment where each base is encoded as a vector of all zeros except one in a specific position. The result of this function is an array numpy.  DNA is a long chain of repeating bases strung together. There are 4 bases: A, C, G, T. For example, \"AACCCAAATCGGGGG\" is a DNA segment.\n",
    "\n",
    "\n",
    "\n",
    "For example, **onehot_dna('AAT')** should return\n",
    "\n",
    "array([[1, 0, 0, 0],\n",
    "       [1, 0, 0, 0],\n",
    "       [0, 0, 0, 1]])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YbhaCZP1moii"
   },
   "outputs": [],
   "source": [
    "def onehot_dna(dna_str):\n",
    "    onehot = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
    "    return [onehot[base] for base in dna_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WM-BaQiGo82A",
    "outputId": "a7357e07-2c79-4d10-be96-aa0d8520b702"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_dna('AAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkSQo19z60n1"
   },
   "source": [
    "# Deep learning to classify Transcription Factor Biding\n",
    "\n",
    "\n",
    "In the next exercises, we will learn how to use Deep learning to predict whether a segment of DNA does include or does not include a sit where JUND binds. (JUND is a particular transcription factor).\n",
    "\n",
    "In this purpose, we will use data that is extracted from the chapter 6 of the book: 'Deep learning for the life science'. This book is written by B.Ramsundar, P.Eastman, P. Walters and V.Pande.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the Wikipedia page of the Jund transcription factor, explain what is the use of such protein "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The use of such protein is to regulate the expression of genes involved in cell proliferation, apoptosis, and transformation. It is a part of the AP-1 transcription factor complex, which is**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Data consist of DNA segments that have been split up from a full chromosome. Each segment is of 101 bases long and has been labeled to indicate whether it does or does not include a site where JUND binds to.\n",
    "\n",
    "\n",
    "This is a binary classification problem.\n",
    "The process of creating a PyTorch neural network binary classifier consists of several steps:\n",
    "\n",
    "1. Prepare the training and test data\n",
    "\n",
    "2. Implement a Dataset object to serve up the data\n",
    "\n",
    "3. Design and implement a neural network\n",
    "\n",
    "4. Write code to train the network\n",
    "\n",
    "5. Write code to evaluate the model (the trained network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS8a5oOmOjqO"
   },
   "source": [
    "# Ex 2:  Load Data\n",
    "\n",
    "The data is available here : https://drive.google.com/drive/folders/1-nrTvNvEZo6Px1pnT7IeotKZR7p365UJ?usp=sharing\n",
    "\n",
    "1. With the help of the joblib library, load the following files for training set:  **y_train.joblib**, **X_train.joblib**  and then store the results in variables **y_train, X_train** ,respectively.\n",
    "\n",
    "2. Do the same thing for the test set: load  **y_test.joblib**, **X_test.joblib**  and then store the results in variables **y_test, X_test**, respectively.\n",
    "\n",
    "3. What are the shape of **X_train** and **y_train** ? How many DNA segments are there in traning set ?\n",
    "\n",
    "4. Display a DNA segment from **X_train** (using matplotlib.pyplot.imshow ).\n",
    "\n",
    "5. Plot the histogram of **y_train** to see whether data is imbalanced or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NB5Y5StQezcZ"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "X_train = joblib.load('tfbind/X_train.joblib')\n",
    "y_train = joblib.load('tfbind/y_train.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = joblib.load('tfbind/X_test.joblib')\n",
    "y_test = joblib.load('tfbind/y_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4672, 101, 4)\n",
      "(4672, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FMLs0OOTV4E5"
   },
   "outputs": [],
   "source": [
    "''' Tests X_train, y_train '''\n",
    "assert(X_train.shape == (4672, 101, 4))\n",
    "assert(y_train.shape ==(4672, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RO7DeSen_W_h"
   },
   "outputs": [],
   "source": [
    "''' Tests X_test, y_test'''\n",
    "assert(X_test.shape == (584, 101, 4))\n",
    "assert(y_test.shape ==(584, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "TceRZcS3MENp",
    "outputId": "25a06914-8dbe-425c-f6eb-d0627b622bac"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EajNPJrnAmde"
   },
   "source": [
    "# Ex 3: Convert numpy array to tensor pytorch\n",
    "\n",
    "As you see in the previous exercise, **X_train** consists of 4672 segments. Each segment is encoded by 0 and 1 (one-hot encoding).\n",
    "\n",
    "\n",
    "1. Convert numpy array **X_train**, **y_train** into pytorch tensor. Reshape **X_train** to (4672, 4, 101). Note that the type of **X_train** and **y_train** should be float.\n",
    "\n",
    "2. Do the same thing for **X_test** and **y_test**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "E8eedW_yPUKu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train_ts = torch.tensor(X_train, dtype=torch.float).permute(0, 2, 1)\n",
    "y_train_ts = torch.tensor(y_train, dtype=torch.float)\n",
    "\n",
    "X_test_ts = torch.tensor(X_test, dtype=torch.float).permute(0, 2, 1)\n",
    "y_test_ts = torch.tensor(y_test, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GrBcHzR-E-yR"
   },
   "outputs": [],
   "source": [
    "''' Tests X_train_ts, y_train_ts '''\n",
    "assert(type(X_train_ts) is torch.Tensor)\n",
    "assert(type(y_train_ts) is torch.Tensor)\n",
    "assert(X_train_ts.shape == (4672, 4, 101))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wn_0dOuVHWhs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "o_it3skAH0Yt"
   },
   "outputs": [],
   "source": [
    "''' Tests the above ToDo. '''\n",
    "assert(type(X_test_ts) is torch.Tensor)\n",
    "assert(type(y_test_ts) is torch.Tensor)\n",
    "assert(X_test_ts.shape == (584, 4, 101))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYJBjBdg547"
   },
   "source": [
    "# Ex4: Create Dataset\n",
    "In order to train a deep learning model with Pytorch, we need a pytorch dataset.\n",
    "The DNADataset class below allows for creating a pytorch Dataset from DNA segments and their labels.\n",
    "\n",
    "1. Using this class, create a dataset for training set. You should call it **train_dataset**\n",
    "\n",
    "2. Create **Dataloader** from **train_dataset**. You should call it **train_loader**.\n",
    "\n",
    "3. Do the same thing for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GkvyOQVggLz_"
   },
   "outputs": [],
   "source": [
    "class DNADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dna, labels):\n",
    "        self.labels = labels\n",
    "        self.dna = dna\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        frag_dna = self.dna[idx]\n",
    "\n",
    "        sample = {'DNA': frag_dna, 'Class': label}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DNADataset(X_train_ts, y_train_ts)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = DNADataset(X_test_ts, y_test_ts)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_d7J-rFbiig"
   },
   "source": [
    "**train_loader** is a generator. To get data out of it, you need to loop through it or convert it to an iterator and call next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJQK0ZaLMpKF",
    "outputId": "76fb9583-e609-4e26-e954-1c88af29b5bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a batch data  torch.Size([32, 4, 101])\n",
      "Shape of label torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Run this to test your data loader\n",
    "data = next(iter(train_loader))\n",
    "dna = data['DNA']\n",
    "label = data['Class']\n",
    "print(\"a batch data \", dna.shape)\n",
    "print(\"Shape of label\", label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDcbP17Rb2xt"
   },
   "source": [
    "# Design and implement a convolutional neural network\n",
    "\n",
    "Now, it's time to build your model. This is a binary classification problem. We can use a convolution neural network, just like an image classification problem. However, since the size of a DNA segment is (4, 101), we will use 1D convolution instead of 2D convolution.\n",
    "\n",
    "\n",
    "\n",
    "Firstly, we will test how does a 1D convolution work on our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlPP4E3UfRYe"
   },
   "source": [
    "# EX 5: 1D Convolution\n",
    "\n",
    "1. With the help of the torch.nn.Conv1d class, create a 1D convolutional layer. You need to choose values for the following parameters: **in_channels**, **out_channels**, **kernel_size**.\n",
    "\n",
    "\n",
    "2. Apply this layer to **dna_seg** below. What is the size of the output ?\n",
    "\n",
    "\n",
    "3. [Optional] Display the output by using matplotlib.pyplot.imshow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 99])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3)\n",
    "out = conv(dna)\n",
    "print(out.shape)\n",
    "\n",
    "plt.imshow(out[0].detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "C9y0yRXhhj1F"
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "dna_seg = data['DNA']\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H26rAy0wkbca"
   },
   "source": [
    "# EX 6: Build a model\n",
    "\n",
    "Create a 3 layer 1-dimensional network to classify the TF binding sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDNA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepDNA, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, kernel_size=8, padding='same')\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=8, padding='same')\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=8, padding='same')\n",
    "        self.lin1 = nn.Linear(6464, 32, bias=True)\n",
    "        self.lin2 = nn.Linear(32, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVbFqM4laclu"
   },
   "source": [
    "# Ex 7 Test the model\n",
    "\n",
    "\n",
    "1. Create an instance of the DeepDNA class named **net**.\n",
    "\n",
    "2. Print out the variable **net** to see detailed information about the model.\n",
    "\n",
    "3. Pass **dna_seg** below to **net** in order to  test if your model **net** works well.\n",
    "\n",
    "4. What is the size of the output ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ladQCir7btvX",
    "outputId": "557662ec-4e25-47aa-dd92-ee56348aeab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the output of the model  tensor([[-0.1726],\n",
      "        [-0.1163],\n",
      "        [-0.1652],\n",
      "        [-0.1378],\n",
      "        [-0.1320]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jubuntu/anaconda3/envs/mlbio/lib/python3.12/site-packages/torch/nn/modules/conv.py:304: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1031.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "#data = next(iter(train_loader))\n",
    "dna_seg = data['DNA']\n",
    "\n",
    "##### TO DO #######\n",
    "net = DeepDNA()\n",
    "out = net(dna_seg)\n",
    "\n",
    "print(\"the output of the model \", out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFzp7DbtlTwG",
    "outputId": "24e73025-a469-4042-b77f-d3d19ba0e2ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepDNA(\n",
      "  (conv1): Conv1d(4, 16, kernel_size=(8,), stride=(1,), padding=same)\n",
      "  (conv2): Conv1d(16, 32, kernel_size=(8,), stride=(1,), padding=same)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(8,), stride=(1,), padding=same)\n",
      "  (lin1): Linear(in_features=6464, out_features=32, bias=True)\n",
      "  (lin2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yypmG11hr_so"
   },
   "source": [
    "# Ex 8: Define loss function and optimizer\n",
    "\n",
    "\n",
    "1. Define an SGD optimizer for the model. You need to choose the learning rate for your model.\n",
    "\n",
    "2. Define a Binary Cross Entropy (BCE) Loss  function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uZwQKSrxh1A_"
   },
   "outputs": [],
   "source": [
    "## TODO ###\n",
    "import numpy as np\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rEjVjhetI6A"
   },
   "source": [
    "# Ex 9: Training your model\n",
    "\n",
    "The following function allows to train the model for one epoch. This function returns total loss per epoch.\n",
    "Implement the training pass for this function.\n",
    "\n",
    "\n",
    "\n",
    "The general process with PyTorch for one learning step consits of several steps:\n",
    "\n",
    "1. Make a forward pass through the network\n",
    "2. Use the network output to calculate the loss\n",
    "3. Perform a backward pass through the network with loss.backward() to calculate the gradients\n",
    "4. Take a step with the optimizer to update the weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oAil9eU9iNCf"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()\n",
    "    loss_epoch = 0\n",
    "    for batch_data in train_loader:\n",
    "        dna = batch_data['DNA']\n",
    "        y = batch_data['Class']\n",
    "        optimizer.zero_grad()\n",
    "        out = net(dna)\n",
    "        out = torch.sigmoid(out)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz0VV2Oenftq"
   },
   "source": [
    "# Ex 11: Accuracy Calculation\n",
    "\n",
    "Write a function named **compute_num_correct_pred(y_prob, y_label)** that allows to compute the number of correct predictions. **y_prob** and **y_label** should be pytorch tensors.\n",
    "\n",
    "For example,\n",
    "y_prob = [[0.3],[0.4], [0.8], [0.7]].\n",
    "\n",
    "y = [[0], [1], [1], [0]].\n",
    "\n",
    "This function should return 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JZ3cRawWpFe7"
   },
   "outputs": [],
   "source": [
    "### TODO ####\n",
    "def compute_num_correct_pred(y_prob, y_label):\n",
    "    \"\"\"\n",
    "    Write a function named **compute_num_correct_pred(y_prob, y_label)** that allows to compute the number of correct predictions. **y_prob** and **y_label** should be pytorch tensors.\n",
    "\n",
    "    For example,\n",
    "    y_prob = [[0.3],[0.4], [0.8], [0.7]].\n",
    "\n",
    "    y = [[0], [1], [1], [0]].\n",
    "\n",
    "    This function should return 2.\n",
    "    \"\"\"\n",
    "    y_pred = (y_prob >= 0.5).int()\n",
    "    correct_predictions = (y_pred == y_label).sum().item()\n",
    "    return correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "y_prob = torch.tensor([[0.3],[0.4], [0.8], [0.7]])\n",
    "y_label = torch.tensor([[0.], [1.], [1.], [0.]])\n",
    "\n",
    "y_prob = torch.sigmoid(y_prob)  # Apply sigmoid to y_prob\n",
    "\n",
    "print(compute_num_correct_pred(y_prob, y_label))\n",
    "assert(compute_num_correct_pred(y_prob, y_label) == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N_g4DA-vex8"
   },
   "source": [
    "\n",
    "The function below allows to calculate the accuracy of the model on dataset loader. Execute this function to see if you implemented the compute_num_correct_pred function correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "FY9V2-NCvv25"
   },
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "  net.eval()\n",
    "\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      dna = data['DNA']\n",
    "      y = data['Class']\n",
    "\n",
    "      out = net(dna)\n",
    "      correct += compute_num_correct_pred(out, y)\n",
    "\n",
    "  return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0vTKeeQ3wGb"
   },
   "source": [
    "# Ex 12: Training the model\n",
    "\n",
    "The code below allows to train your model on 10 epoches. If all work well, you should see the training loss drop with each epoch.\n",
    "\n",
    "train the model for 2000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 90.8931342959404, Train acc 0.75, Test acc 0.75\n",
      "Epoch 1, Loss 84.45528274774551, Train acc 0.75, Test acc 0.75\n",
      "Epoch 2, Loss 82.41789615154266, Train acc 0.75, Test acc 0.75\n",
      "Epoch 3, Loss 82.09645494818687, Train acc 0.75, Test acc 0.75\n",
      "Epoch 4, Loss 82.05190601944923, Train acc 0.75, Test acc 0.75\n",
      "Epoch 5, Loss 82.02702695131302, Train acc 0.75, Test acc 0.75\n",
      "Epoch 6, Loss 82.02330708503723, Train acc 0.75, Test acc 0.75\n",
      "Epoch 7, Loss 82.01029083132744, Train acc 0.75, Test acc 0.75\n",
      "Epoch 8, Loss 81.99428445100784, Train acc 0.75, Test acc 0.75\n",
      "Epoch 9, Loss 81.98903748393059, Train acc 0.75, Test acc 0.75\n",
      "Epoch 10, Loss 81.97178944945335, Train acc 0.75, Test acc 0.75\n",
      "Epoch 11, Loss 81.95259526371956, Train acc 0.75, Test acc 0.75\n",
      "Epoch 12, Loss 81.94163286685944, Train acc 0.75, Test acc 0.75\n",
      "Epoch 13, Loss 81.9313588142395, Train acc 0.75, Test acc 0.75\n",
      "Epoch 14, Loss 81.91648828983307, Train acc 0.75, Test acc 0.75\n",
      "Epoch 15, Loss 81.90326717495918, Train acc 0.75, Test acc 0.75\n",
      "Epoch 16, Loss 81.88858953118324, Train acc 0.75, Test acc 0.75\n",
      "Epoch 17, Loss 81.8786293566227, Train acc 0.75, Test acc 0.75\n",
      "Epoch 18, Loss 81.84677684307098, Train acc 0.75, Test acc 0.75\n",
      "Epoch 19, Loss 81.85199224948883, Train acc 0.75, Test acc 0.75\n",
      "Epoch 20, Loss 81.83609423041344, Train acc 0.75, Test acc 0.75\n",
      "Epoch 21, Loss 81.8302570283413, Train acc 0.75, Test acc 0.75\n",
      "Epoch 22, Loss 81.81048348546028, Train acc 0.75, Test acc 0.75\n",
      "Epoch 23, Loss 81.79856726527214, Train acc 0.75, Test acc 0.75\n",
      "Epoch 24, Loss 81.78801015019417, Train acc 0.75, Test acc 0.75\n",
      "Epoch 25, Loss 81.7751727104187, Train acc 0.75, Test acc 0.75\n",
      "Epoch 26, Loss 81.76176169514656, Train acc 0.75, Test acc 0.75\n",
      "Epoch 27, Loss 81.74495965242386, Train acc 0.75, Test acc 0.75\n",
      "Epoch 28, Loss 81.73130589723587, Train acc 0.75, Test acc 0.75\n",
      "Epoch 29, Loss 81.72440066933632, Train acc 0.75, Test acc 0.75\n",
      "Epoch 30, Loss 81.70573371648788, Train acc 0.75, Test acc 0.75\n",
      "Epoch 31, Loss 81.7012922167778, Train acc 0.75, Test acc 0.75\n",
      "Epoch 32, Loss 81.68235892057419, Train acc 0.75, Test acc 0.75\n",
      "Epoch 33, Loss 81.67179071903229, Train acc 0.75, Test acc 0.75\n",
      "Epoch 34, Loss 81.65309017896652, Train acc 0.75, Test acc 0.75\n",
      "Epoch 35, Loss 81.62350106239319, Train acc 0.75, Test acc 0.75\n",
      "Epoch 36, Loss 81.64128565788269, Train acc 0.75, Test acc 0.75\n",
      "Epoch 37, Loss 81.61976072192192, Train acc 0.75, Test acc 0.75\n",
      "Epoch 38, Loss 81.60440963506699, Train acc 0.75, Test acc 0.75\n",
      "Epoch 39, Loss 81.5943620800972, Train acc 0.75, Test acc 0.75\n",
      "Epoch 40, Loss 81.58097466826439, Train acc 0.75, Test acc 0.75\n",
      "Epoch 41, Loss 81.56629478931427, Train acc 0.75, Test acc 0.75\n",
      "Epoch 42, Loss 81.54776111245155, Train acc 0.75, Test acc 0.75\n",
      "Epoch 43, Loss 81.54579466581345, Train acc 0.75, Test acc 0.75\n",
      "Epoch 44, Loss 81.52773573994637, Train acc 0.75, Test acc 0.75\n",
      "Epoch 45, Loss 81.511415630579, Train acc 0.75, Test acc 0.75\n",
      "Epoch 46, Loss 81.50304162502289, Train acc 0.75, Test acc 0.75\n",
      "Epoch 47, Loss 81.48156309127808, Train acc 0.75, Test acc 0.75\n",
      "Epoch 48, Loss 81.47871431708336, Train acc 0.75, Test acc 0.75\n",
      "Epoch 49, Loss 81.45778942108154, Train acc 0.75, Test acc 0.75\n",
      "Epoch 50, Loss 81.44581803679466, Train acc 0.75, Test acc 0.75\n",
      "Epoch 51, Loss 81.43227180838585, Train acc 0.75, Test acc 0.75\n",
      "Epoch 52, Loss 81.41873148083687, Train acc 0.75, Test acc 0.75\n",
      "Epoch 53, Loss 81.40885379910469, Train acc 0.75, Test acc 0.75\n",
      "Epoch 54, Loss 81.39251139760017, Train acc 0.75, Test acc 0.75\n",
      "Epoch 55, Loss 81.37630498409271, Train acc 0.75, Test acc 0.75\n",
      "Epoch 56, Loss 81.36475449800491, Train acc 0.75, Test acc 0.75\n",
      "Epoch 57, Loss 81.35285264253616, Train acc 0.75, Test acc 0.75\n",
      "Epoch 58, Loss 81.33861231803894, Train acc 0.75, Test acc 0.75\n",
      "Epoch 59, Loss 81.32292520999908, Train acc 0.75, Test acc 0.75\n",
      "Epoch 60, Loss 81.30775928497314, Train acc 0.75, Test acc 0.75\n",
      "Epoch 61, Loss 81.28042936325073, Train acc 0.75, Test acc 0.75\n",
      "Epoch 62, Loss 81.28287237882614, Train acc 0.75, Test acc 0.75\n",
      "Epoch 63, Loss 81.26791855692863, Train acc 0.75, Test acc 0.75\n",
      "Epoch 64, Loss 81.25392979383469, Train acc 0.75, Test acc 0.75\n",
      "Epoch 65, Loss 81.23188650608063, Train acc 0.75, Test acc 0.75\n",
      "Epoch 66, Loss 81.22554713487625, Train acc 0.75, Test acc 0.75\n",
      "Epoch 67, Loss 81.20834693312645, Train acc 0.75, Test acc 0.75\n",
      "Epoch 68, Loss 81.19940829277039, Train acc 0.75, Test acc 0.75\n",
      "Epoch 69, Loss 81.1654591858387, Train acc 0.75, Test acc 0.75\n",
      "Epoch 70, Loss 81.16218709945679, Train acc 0.75, Test acc 0.75\n",
      "Epoch 71, Loss 81.15889182686806, Train acc 0.75, Test acc 0.75\n",
      "Epoch 72, Loss 81.13730150461197, Train acc 0.75, Test acc 0.75\n",
      "Epoch 73, Loss 81.12383836507797, Train acc 0.75, Test acc 0.75\n",
      "Epoch 74, Loss 81.1096513569355, Train acc 0.75, Test acc 0.75\n",
      "Epoch 75, Loss 81.0925752222538, Train acc 0.75, Test acc 0.75\n",
      "Epoch 76, Loss 81.07793352007866, Train acc 0.75, Test acc 0.75\n",
      "Epoch 77, Loss 81.06128323078156, Train acc 0.75, Test acc 0.75\n",
      "Epoch 78, Loss 81.04343938827515, Train acc 0.75, Test acc 0.75\n",
      "Epoch 79, Loss 81.0324430167675, Train acc 0.75, Test acc 0.75\n",
      "Epoch 80, Loss 81.02141934633255, Train acc 0.75, Test acc 0.75\n",
      "Epoch 81, Loss 81.00245279073715, Train acc 0.75, Test acc 0.75\n",
      "Epoch 82, Loss 80.96640005707741, Train acc 0.75, Test acc 0.75\n",
      "Epoch 83, Loss 80.9731929898262, Train acc 0.75, Test acc 0.75\n",
      "Epoch 84, Loss 80.95020025968552, Train acc 0.75, Test acc 0.75\n",
      "Epoch 85, Loss 80.93260711431503, Train acc 0.75, Test acc 0.75\n",
      "Epoch 86, Loss 80.92516702413559, Train acc 0.75, Test acc 0.75\n",
      "Epoch 87, Loss 80.89756897091866, Train acc 0.75, Test acc 0.75\n",
      "Epoch 88, Loss 80.8858707845211, Train acc 0.75, Test acc 0.75\n",
      "Epoch 89, Loss 80.86984032392502, Train acc 0.75, Test acc 0.75\n",
      "Epoch 90, Loss 80.85914695262909, Train acc 0.75, Test acc 0.75\n",
      "Epoch 91, Loss 80.84109517931938, Train acc 0.75, Test acc 0.75\n",
      "Epoch 92, Loss 80.82330197095871, Train acc 0.75, Test acc 0.75\n",
      "Epoch 93, Loss 80.80045762658119, Train acc 0.75, Test acc 0.75\n",
      "Epoch 94, Loss 80.79000934958458, Train acc 0.75, Test acc 0.75\n",
      "Epoch 95, Loss 80.77353191375732, Train acc 0.75, Test acc 0.75\n",
      "Epoch 96, Loss 80.75169175863266, Train acc 0.75, Test acc 0.75\n",
      "Epoch 97, Loss 80.74009808897972, Train acc 0.75, Test acc 0.75\n",
      "Epoch 98, Loss 80.72106754779816, Train acc 0.75, Test acc 0.75\n",
      "Epoch 99, Loss 80.70002648234367, Train acc 0.75, Test acc 0.75\n",
      "Epoch 100, Loss 80.6823735833168, Train acc 0.75, Test acc 0.75\n",
      "Epoch 101, Loss 80.66775980591774, Train acc 0.75, Test acc 0.75\n",
      "Epoch 102, Loss 80.6497690975666, Train acc 0.75, Test acc 0.75\n",
      "Epoch 103, Loss 80.63245785236359, Train acc 0.75, Test acc 0.75\n",
      "Epoch 104, Loss 80.6138109266758, Train acc 0.75, Test acc 0.75\n",
      "Epoch 105, Loss 80.59890761971474, Train acc 0.75, Test acc 0.75\n",
      "Epoch 106, Loss 80.58333158493042, Train acc 0.75, Test acc 0.75\n",
      "Epoch 107, Loss 80.55595278739929, Train acc 0.75, Test acc 0.75\n",
      "Epoch 108, Loss 80.54589441418648, Train acc 0.75, Test acc 0.75\n",
      "Epoch 109, Loss 80.53159910440445, Train acc 0.75, Test acc 0.75\n",
      "Epoch 110, Loss 80.50853317975998, Train acc 0.75, Test acc 0.75\n",
      "Epoch 111, Loss 80.48889037966728, Train acc 0.75, Test acc 0.75\n",
      "Epoch 112, Loss 80.47066822648048, Train acc 0.75, Test acc 0.75\n",
      "Epoch 113, Loss 80.44952055811882, Train acc 0.75, Test acc 0.75\n",
      "Epoch 114, Loss 80.43286818265915, Train acc 0.75, Test acc 0.75\n",
      "Epoch 115, Loss 80.41644486784935, Train acc 0.75, Test acc 0.75\n",
      "Epoch 116, Loss 80.38826316595078, Train acc 0.75, Test acc 0.75\n",
      "Epoch 117, Loss 80.36271440982819, Train acc 0.75, Test acc 0.75\n",
      "Epoch 118, Loss 80.34749463200569, Train acc 0.75, Test acc 0.75\n",
      "Epoch 119, Loss 80.32890000939369, Train acc 0.75, Test acc 0.75\n",
      "Epoch 120, Loss 80.30757230520248, Train acc 0.75, Test acc 0.75\n",
      "Epoch 121, Loss 80.29785513877869, Train acc 0.75, Test acc 0.75\n",
      "Epoch 122, Loss 80.28195184469223, Train acc 0.75, Test acc 0.75\n",
      "Epoch 123, Loss 80.25321018695831, Train acc 0.75, Test acc 0.75\n",
      "Epoch 124, Loss 80.23366382718086, Train acc 0.75, Test acc 0.75\n",
      "Epoch 125, Loss 80.21283853054047, Train acc 0.75, Test acc 0.75\n",
      "Epoch 126, Loss 80.20004889369011, Train acc 0.75, Test acc 0.75\n",
      "Epoch 127, Loss 80.17096772789955, Train acc 0.75, Test acc 0.75\n",
      "Epoch 128, Loss 80.15532648563385, Train acc 0.75, Test acc 0.75\n",
      "Epoch 129, Loss 80.1358048915863, Train acc 0.75, Test acc 0.75\n",
      "Epoch 130, Loss 80.11301162838936, Train acc 0.75, Test acc 0.75\n",
      "Epoch 131, Loss 80.09740945696831, Train acc 0.75, Test acc 0.75\n",
      "Epoch 132, Loss 80.06116384267807, Train acc 0.75, Test acc 0.75\n",
      "Epoch 133, Loss 80.0555250942707, Train acc 0.75, Test acc 0.75\n",
      "Epoch 134, Loss 80.0250808596611, Train acc 0.75, Test acc 0.75\n",
      "Epoch 135, Loss 80.01571187376976, Train acc 0.75, Test acc 0.75\n",
      "Epoch 136, Loss 79.99430051445961, Train acc 0.75, Test acc 0.75\n",
      "Epoch 137, Loss 79.97439289093018, Train acc 0.75, Test acc 0.75\n",
      "Epoch 138, Loss 79.94396913051605, Train acc 0.75, Test acc 0.75\n",
      "Epoch 139, Loss 79.92662480473518, Train acc 0.75, Test acc 0.75\n",
      "Epoch 140, Loss 79.9041077196598, Train acc 0.75, Test acc 0.75\n",
      "Epoch 141, Loss 79.87397754192352, Train acc 0.75, Test acc 0.75\n",
      "Epoch 142, Loss 79.85940599441528, Train acc 0.75, Test acc 0.75\n",
      "Epoch 143, Loss 79.84288504719734, Train acc 0.75, Test acc 0.75\n",
      "Epoch 144, Loss 79.81925493478775, Train acc 0.75, Test acc 0.75\n",
      "Epoch 145, Loss 79.79568848013878, Train acc 0.75, Test acc 0.75\n",
      "Epoch 146, Loss 79.78172519803047, Train acc 0.75, Test acc 0.75\n",
      "Epoch 147, Loss 79.74993544816971, Train acc 0.75, Test acc 0.75\n",
      "Epoch 148, Loss 79.73378938436508, Train acc 0.75, Test acc 0.75\n",
      "Epoch 149, Loss 79.71356847882271, Train acc 0.75, Test acc 0.75\n",
      "Epoch 150, Loss 79.6891875565052, Train acc 0.75, Test acc 0.75\n",
      "Epoch 151, Loss 79.66459661722183, Train acc 0.75, Test acc 0.75\n",
      "Epoch 152, Loss 79.64447033405304, Train acc 0.75, Test acc 0.75\n",
      "Epoch 153, Loss 79.61995446681976, Train acc 0.75, Test acc 0.75\n",
      "Epoch 154, Loss 79.61097511649132, Train acc 0.75, Test acc 0.75\n",
      "Epoch 155, Loss 79.58113631606102, Train acc 0.75, Test acc 0.75\n",
      "Epoch 156, Loss 79.56132823228836, Train acc 0.75, Test acc 0.75\n",
      "Epoch 157, Loss 79.54102456569672, Train acc 0.75, Test acc 0.75\n",
      "Epoch 158, Loss 79.52396011352539, Train acc 0.75, Test acc 0.75\n",
      "Epoch 159, Loss 79.4895530641079, Train acc 0.75, Test acc 0.75\n",
      "Epoch 160, Loss 79.45189362764359, Train acc 0.75, Test acc 0.75\n",
      "Epoch 161, Loss 79.44375070929527, Train acc 0.75, Test acc 0.75\n",
      "Epoch 162, Loss 79.42838445305824, Train acc 0.75, Test acc 0.75\n",
      "Epoch 163, Loss 79.40505954623222, Train acc 0.75, Test acc 0.75\n",
      "Epoch 164, Loss 79.39012563228607, Train acc 0.75, Test acc 0.75\n",
      "Epoch 165, Loss 79.36612305045128, Train acc 0.75, Test acc 0.75\n",
      "Epoch 166, Loss 79.34161800146103, Train acc 0.75, Test acc 0.75\n",
      "Epoch 167, Loss 79.32242712378502, Train acc 0.75, Test acc 0.75\n",
      "Epoch 168, Loss 79.30649217963219, Train acc 0.75, Test acc 0.75\n",
      "Epoch 169, Loss 79.26505970954895, Train acc 0.75, Test acc 0.75\n",
      "Epoch 170, Loss 79.25259903073311, Train acc 0.75, Test acc 0.75\n",
      "Epoch 171, Loss 79.23554396629333, Train acc 0.75, Test acc 0.75\n",
      "Epoch 172, Loss 79.21090757846832, Train acc 0.75, Test acc 0.75\n",
      "Epoch 173, Loss 79.19346204400063, Train acc 0.75, Test acc 0.75\n",
      "Epoch 174, Loss 79.15316873788834, Train acc 0.75, Test acc 0.75\n",
      "Epoch 175, Loss 79.16043728590012, Train acc 0.75, Test acc 0.75\n",
      "Epoch 176, Loss 79.13473454117775, Train acc 0.75, Test acc 0.75\n",
      "Epoch 177, Loss 79.09836170077324, Train acc 0.75, Test acc 0.75\n",
      "Epoch 178, Loss 79.09574830532074, Train acc 0.75, Test acc 0.75\n",
      "Epoch 179, Loss 79.05706140398979, Train acc 0.75, Test acc 0.75\n",
      "Epoch 180, Loss 79.04905599355698, Train acc 0.75, Test acc 0.75\n",
      "Epoch 181, Loss 79.02758014202118, Train acc 0.75, Test acc 0.75\n",
      "Epoch 182, Loss 79.00640368461609, Train acc 0.75, Test acc 0.75\n",
      "Epoch 183, Loss 78.98175179958344, Train acc 0.75, Test acc 0.75\n",
      "Epoch 184, Loss 78.97375640273094, Train acc 0.75, Test acc 0.75\n",
      "Epoch 185, Loss 78.94118592143059, Train acc 0.75, Test acc 0.75\n",
      "Epoch 186, Loss 78.92602482438087, Train acc 0.75, Test acc 0.75\n",
      "Epoch 187, Loss 78.90402820706367, Train acc 0.75, Test acc 0.75\n",
      "Epoch 188, Loss 78.89187180995941, Train acc 0.75, Test acc 0.75\n",
      "Epoch 189, Loss 78.86769995093346, Train acc 0.75, Test acc 0.75\n",
      "Epoch 190, Loss 78.84897497296333, Train acc 0.75, Test acc 0.75\n",
      "Epoch 191, Loss 78.82790169119835, Train acc 0.75, Test acc 0.75\n",
      "Epoch 192, Loss 78.81520712375641, Train acc 0.75, Test acc 0.75\n",
      "Epoch 193, Loss 78.79125809669495, Train acc 0.75, Test acc 0.75\n",
      "Epoch 194, Loss 78.7682885825634, Train acc 0.75, Test acc 0.75\n",
      "Epoch 195, Loss 78.75410977005959, Train acc 0.75, Test acc 0.75\n",
      "Epoch 196, Loss 78.74615770578384, Train acc 0.75, Test acc 0.75\n",
      "Epoch 197, Loss 78.72082105278969, Train acc 0.75, Test acc 0.75\n",
      "Epoch 198, Loss 78.71043527126312, Train acc 0.75, Test acc 0.75\n",
      "Epoch 199, Loss 78.68544605374336, Train acc 0.75, Test acc 0.75\n",
      "Epoch 200, Loss 78.66498297452927, Train acc 0.75, Test acc 0.75\n",
      "Epoch 201, Loss 78.637162566185, Train acc 0.75, Test acc 0.75\n",
      "Epoch 202, Loss 78.63024407625198, Train acc 0.75, Test acc 0.75\n",
      "Epoch 203, Loss 78.60965847969055, Train acc 0.75, Test acc 0.75\n",
      "Epoch 204, Loss 78.57675391435623, Train acc 0.75, Test acc 0.75\n",
      "Epoch 205, Loss 78.58361986279488, Train acc 0.75, Test acc 0.75\n",
      "Epoch 206, Loss 78.56170058250427, Train acc 0.75, Test acc 0.75\n",
      "Epoch 207, Loss 78.55166059732437, Train acc 0.75, Test acc 0.75\n",
      "Epoch 208, Loss 78.53485995531082, Train acc 0.75, Test acc 0.75\n",
      "Epoch 209, Loss 78.5213879942894, Train acc 0.75, Test acc 0.75\n",
      "Epoch 210, Loss 78.49544140696526, Train acc 0.75, Test acc 0.75\n",
      "Epoch 211, Loss 78.48591235280037, Train acc 0.75, Test acc 0.75\n",
      "Epoch 212, Loss 78.47364476323128, Train acc 0.75, Test acc 0.75\n",
      "Epoch 213, Loss 78.46775969862938, Train acc 0.75, Test acc 0.75\n",
      "Epoch 214, Loss 78.42939466238022, Train acc 0.75, Test acc 0.75\n",
      "Epoch 215, Loss 78.43142884969711, Train acc 0.75, Test acc 0.75\n",
      "Epoch 216, Loss 78.40611612796783, Train acc 0.75, Test acc 0.75\n",
      "Epoch 217, Loss 78.40549957752228, Train acc 0.75, Test acc 0.75\n",
      "Epoch 218, Loss 78.3727899491787, Train acc 0.75, Test acc 0.75\n",
      "Epoch 219, Loss 78.37220898270607, Train acc 0.75, Test acc 0.75\n",
      "Epoch 220, Loss 78.3553835451603, Train acc 0.75, Test acc 0.75\n",
      "Epoch 221, Loss 78.3396415412426, Train acc 0.75, Test acc 0.75\n",
      "Epoch 222, Loss 78.32389006018639, Train acc 0.75, Test acc 0.75\n",
      "Epoch 223, Loss 78.3106555044651, Train acc 0.75, Test acc 0.75\n",
      "Epoch 224, Loss 78.30328387022018, Train acc 0.75, Test acc 0.75\n",
      "Epoch 225, Loss 78.29975661635399, Train acc 0.75, Test acc 0.75\n",
      "Epoch 226, Loss 78.2539736032486, Train acc 0.75, Test acc 0.75\n",
      "Epoch 227, Loss 78.27255275845528, Train acc 0.75, Test acc 0.75\n",
      "Epoch 228, Loss 78.25488302111626, Train acc 0.75, Test acc 0.75\n",
      "Epoch 229, Loss 78.24353659152985, Train acc 0.75, Test acc 0.75\n",
      "Epoch 230, Loss 78.22398963570595, Train acc 0.75, Test acc 0.75\n",
      "Epoch 231, Loss 78.21585407853127, Train acc 0.75, Test acc 0.75\n",
      "Epoch 232, Loss 78.20421546697617, Train acc 0.75, Test acc 0.75\n",
      "Epoch 233, Loss 78.19197553396225, Train acc 0.75, Test acc 0.75\n",
      "Epoch 234, Loss 78.17721325159073, Train acc 0.75, Test acc 0.75\n",
      "Epoch 235, Loss 78.1662865281105, Train acc 0.75, Test acc 0.75\n",
      "Epoch 236, Loss 78.16541519761086, Train acc 0.75, Test acc 0.75\n",
      "Epoch 237, Loss 78.1602635383606, Train acc 0.75, Test acc 0.75\n",
      "Epoch 238, Loss 78.14503428339958, Train acc 0.75, Test acc 0.75\n",
      "Epoch 239, Loss 78.12754556536674, Train acc 0.75, Test acc 0.75\n",
      "Epoch 240, Loss 78.11856305599213, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 241, Loss 78.10650438070297, Train acc 0.75, Test acc 0.75\n",
      "Epoch 242, Loss 78.09572058916092, Train acc 0.75, Test acc 0.75\n",
      "Epoch 243, Loss 78.09818068146706, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 244, Loss 78.0700135231018, Train acc 0.75, Test acc 0.75\n",
      "Epoch 245, Loss 78.08371362090111, Train acc 0.75, Test acc 0.75\n",
      "Epoch 246, Loss 78.06083169579506, Train acc 0.75, Test acc 0.75\n",
      "Epoch 247, Loss 78.05748948454857, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 248, Loss 78.04687711596489, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 249, Loss 78.03282597661018, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 250, Loss 78.03407597541809, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 251, Loss 78.00704389810562, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 252, Loss 78.02883261442184, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 253, Loss 77.9919521510601, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 254, Loss 77.9915736913681, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 255, Loss 77.98424437642097, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 256, Loss 77.98343469202518, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 257, Loss 77.96042138338089, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 258, Loss 77.96174716949463, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 259, Loss 77.95438480377197, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 260, Loss 77.9388900399208, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 261, Loss 77.94534328579903, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 262, Loss 77.93211910128593, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 263, Loss 77.93000420928001, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 264, Loss 77.91623595356941, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 265, Loss 77.91685333848, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 266, Loss 77.89608472585678, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 267, Loss 77.90193927288055, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 268, Loss 77.8956758081913, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 269, Loss 77.88259917497635, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 270, Loss 77.87346863746643, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 271, Loss 77.86449012160301, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 272, Loss 77.83987841010094, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 273, Loss 77.86717894673347, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 274, Loss 77.8464363515377, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 275, Loss 77.84677070379257, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 276, Loss 77.83671608567238, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 277, Loss 77.83347672224045, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 278, Loss 77.83140695095062, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 279, Loss 77.81980693340302, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 280, Loss 77.81573683023453, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 281, Loss 77.81428617238998, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 282, Loss 77.79969125986099, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 283, Loss 77.79548099637032, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 284, Loss 77.79341903328896, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 285, Loss 77.77921050786972, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 286, Loss 77.79099896550179, Train acc 0.75, Test acc 0.75\n",
      "Epoch 287, Loss 77.78014448285103, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 288, Loss 77.78151631355286, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 289, Loss 77.76226904988289, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 290, Loss 77.77551007270813, Train acc 0.75, Test acc 0.75\n",
      "Epoch 291, Loss 77.75905698537827, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 292, Loss 77.75932791829109, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 293, Loss 77.73872309923172, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 294, Loss 77.73263865709305, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 295, Loss 77.7487151324749, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 296, Loss 77.73669305443764, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 297, Loss 77.73801761865616, Train acc 0.75, Test acc 0.75\n",
      "Epoch 298, Loss 77.71573448181152, Train acc 0.75, Test acc 0.75\n",
      "Epoch 299, Loss 77.72302114963531, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 300, Loss 77.72140204906464, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 301, Loss 77.71478056907654, Train acc 0.75, Test acc 0.75\n",
      "Epoch 302, Loss 77.70047172904015, Train acc 0.75, Test acc 0.75\n",
      "Epoch 303, Loss 77.70443883538246, Train acc 0.75, Test acc 0.75\n",
      "Epoch 304, Loss 77.66810309886932, Train acc 0.75, Test acc 0.75\n",
      "Epoch 305, Loss 77.6924116909504, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 306, Loss 77.69247102737427, Train acc 0.75, Test acc 0.75\n",
      "Epoch 307, Loss 77.69313287734985, Train acc 0.75, Test acc 0.75\n",
      "Epoch 308, Loss 77.66646376252174, Train acc 0.75, Test acc 0.75\n",
      "Epoch 309, Loss 77.68321028351784, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 310, Loss 77.67444959282875, Train acc 0.75, Test acc 0.75\n",
      "Epoch 311, Loss 77.66162303090096, Train acc 0.75, Test acc 0.75\n",
      "Epoch 312, Loss 77.67263492941856, Train acc 0.75, Test acc 0.75\n",
      "Epoch 313, Loss 77.65418887138367, Train acc 0.75, Test acc 0.75\n",
      "Epoch 314, Loss 77.6464355289936, Train acc 0.75, Test acc 0.75\n",
      "Epoch 315, Loss 77.637570977211, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 316, Loss 77.65765577554703, Train acc 0.75, Test acc 0.75\n",
      "Epoch 317, Loss 77.62185621261597, Train acc 0.75, Test acc 0.75\n",
      "Epoch 318, Loss 77.65279331803322, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 319, Loss 77.6275372505188, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 320, Loss 77.63535806536674, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 321, Loss 77.64343976974487, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 322, Loss 77.62011203169823, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 323, Loss 77.63088953495026, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 324, Loss 77.604831635952, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 325, Loss 77.60840520262718, Train acc 0.7506421232876712, Test acc 0.75\n",
      "Epoch 326, Loss 77.61734366416931, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 327, Loss 77.60888403654099, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 328, Loss 77.60562452673912, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 329, Loss 77.60252067446709, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 330, Loss 77.60628041625023, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 331, Loss 77.59596940875053, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 332, Loss 77.59208384156227, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 333, Loss 77.59267336130142, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 334, Loss 77.57553935050964, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 335, Loss 77.58365398645401, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 336, Loss 77.57934704422951, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 337, Loss 77.55759823322296, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 338, Loss 77.57319188117981, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 339, Loss 77.55737373232841, Train acc 0.7506421232876712, Test acc 0.75\n",
      "Epoch 340, Loss 77.55832123756409, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 341, Loss 77.56445559859276, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 342, Loss 77.55412483215332, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 343, Loss 77.55471208691597, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 344, Loss 77.55327594280243, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 345, Loss 77.5417383313179, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 346, Loss 77.54561525583267, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 347, Loss 77.53870728611946, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 348, Loss 77.54791709780693, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 349, Loss 77.53312104940414, Train acc 0.7506421232876712, Test acc 0.75\n",
      "Epoch 350, Loss 77.54440021514893, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 351, Loss 77.53452360630035, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 352, Loss 77.52016654610634, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 353, Loss 77.51960164308548, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 354, Loss 77.52526018023491, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 355, Loss 77.53122186660767, Train acc 0.7506421232876712, Test acc 0.75\n",
      "Epoch 356, Loss 77.5036995112896, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 357, Loss 77.51183599233627, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 358, Loss 77.48634585738182, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 359, Loss 77.5076978802681, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 360, Loss 77.50416114926338, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 361, Loss 77.51145002245903, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 362, Loss 77.50251218676567, Train acc 0.7506421232876712, Test acc 0.75\n",
      "Epoch 363, Loss 77.494608938694, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 364, Loss 77.48122316598892, Train acc 0.7506421232876712, Test acc 0.75\n",
      "Epoch 365, Loss 77.5023105442524, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 366, Loss 77.49826493859291, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 367, Loss 77.49174481630325, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 368, Loss 77.48898413777351, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 369, Loss 77.48954510688782, Train acc 0.7506421232876712, Test acc 0.75\n",
      "Epoch 370, Loss 77.47518974542618, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 371, Loss 77.49441853165627, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 372, Loss 77.47813603281975, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 373, Loss 77.46970850229263, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 374, Loss 77.48022118210793, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 375, Loss 77.47575759887695, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 376, Loss 77.47351962327957, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 377, Loss 77.46345666050911, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 378, Loss 77.46141114830971, Train acc 0.7504280821917808, Test acc 0.75\n",
      "Epoch 379, Loss 77.47290125489235, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 380, Loss 77.46326875686646, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 381, Loss 77.46301847696304, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 382, Loss 77.44556707143784, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 383, Loss 77.47178712487221, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 384, Loss 77.44266444444656, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 385, Loss 77.39585694670677, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 386, Loss 77.43652388453484, Train acc 0.7504280821917808, Test acc 0.7465753424657534\n",
      "Epoch 387, Loss 77.45352318882942, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 388, Loss 77.44360968470573, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 389, Loss 77.44971179962158, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 390, Loss 77.43875473737717, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 391, Loss 77.44985544681549, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 392, Loss 77.429796397686, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 393, Loss 77.44786760210991, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 394, Loss 77.44224885106087, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 395, Loss 77.43001651763916, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 396, Loss 77.41469922661781, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 397, Loss 77.42758411169052, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 398, Loss 77.43344217538834, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 399, Loss 77.43872803449631, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 400, Loss 77.42666020989418, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 401, Loss 77.42565321922302, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 402, Loss 77.42710915207863, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 403, Loss 77.4063158929348, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 404, Loss 77.41308426856995, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 405, Loss 77.40914848446846, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 406, Loss 77.41753458976746, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 407, Loss 77.41035941243172, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 408, Loss 77.41446009278297, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 409, Loss 77.41311532258987, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 410, Loss 77.40971773862839, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 411, Loss 77.40673729777336, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 412, Loss 77.40918773412704, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 413, Loss 77.4038907289505, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 414, Loss 77.40021666884422, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 415, Loss 77.3971458375454, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 416, Loss 77.40379232168198, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 417, Loss 77.40163737535477, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 418, Loss 77.38827347755432, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 419, Loss 77.38884806632996, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 420, Loss 77.40522563457489, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 421, Loss 77.38506352901459, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 422, Loss 77.37577041983604, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 423, Loss 77.39703199267387, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 424, Loss 77.39378294348717, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 425, Loss 77.38871231675148, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 426, Loss 77.38543230295181, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 427, Loss 77.39897474646568, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 428, Loss 77.38038325309753, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 429, Loss 77.3871243596077, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 430, Loss 77.38866811990738, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 431, Loss 77.34543591737747, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 432, Loss 77.39768567681313, Train acc 0.7502140410958904, Test acc 0.75\n",
      "Epoch 433, Loss 77.38084852695465, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 434, Loss 77.37989792227745, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 435, Loss 77.37455615401268, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 436, Loss 77.38041964173317, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 437, Loss 77.37771591544151, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 438, Loss 77.38312977552414, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 439, Loss 77.37574350833893, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 440, Loss 77.33410939574242, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 441, Loss 77.37228313088417, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 442, Loss 77.37903568148613, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 443, Loss 77.36631348729134, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 444, Loss 77.37404996156693, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 445, Loss 77.36716827750206, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 446, Loss 77.35996681451797, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 447, Loss 77.36533153057098, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 448, Loss 77.3592172563076, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 449, Loss 77.34424570202827, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 450, Loss 77.37431445717812, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 451, Loss 77.35890284180641, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 452, Loss 77.35743913054466, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 453, Loss 77.36596459150314, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 454, Loss 77.361667573452, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 455, Loss 77.36840483546257, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 456, Loss 77.35269919037819, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 457, Loss 77.33941939473152, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 458, Loss 77.3600492477417, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 459, Loss 77.34875655174255, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 460, Loss 77.35781818628311, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 461, Loss 77.35954171419144, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 462, Loss 77.34358975291252, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 463, Loss 77.33752033114433, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 464, Loss 77.33114767074585, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 465, Loss 77.33631956577301, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 466, Loss 77.34604555368423, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 467, Loss 77.34192907810211, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 468, Loss 77.35135498642921, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 469, Loss 77.33678194880486, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 470, Loss 77.34309411048889, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 471, Loss 77.3484436571598, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 472, Loss 77.3345816731453, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 473, Loss 77.35005408525467, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 474, Loss 77.33334678411484, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 475, Loss 77.33620834350586, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 476, Loss 77.34402710199356, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 477, Loss 77.3336007297039, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 478, Loss 77.34049588441849, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 479, Loss 77.3178722858429, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 480, Loss 77.35159915685654, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 481, Loss 77.31609013676643, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 482, Loss 77.33730563521385, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 483, Loss 77.32979476451874, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 484, Loss 77.33370506763458, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 485, Loss 77.3368488252163, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 486, Loss 77.30484494566917, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 487, Loss 77.32821950316429, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 488, Loss 77.33440431952477, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 489, Loss 77.32168278098106, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 490, Loss 77.33648362755775, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 491, Loss 77.32359647750854, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 492, Loss 77.33239993453026, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 493, Loss 77.32957211136818, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 494, Loss 77.33067515492439, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 495, Loss 77.33360782265663, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 496, Loss 77.33246931433678, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 497, Loss 77.32401859760284, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 498, Loss 77.32894486188889, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 499, Loss 77.32624220848083, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 500, Loss 77.3264673948288, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 501, Loss 77.33271962404251, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 502, Loss 77.31171026825905, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 503, Loss 77.33202168345451, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 504, Loss 77.30895441770554, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 505, Loss 77.33850798010826, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 506, Loss 77.31989008188248, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 507, Loss 77.32545071840286, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 508, Loss 77.33073657751083, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 509, Loss 77.3133727312088, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 510, Loss 77.31574380397797, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 511, Loss 77.33020505309105, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 512, Loss 77.32373028993607, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 513, Loss 77.3068815767765, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 514, Loss 77.32701995968819, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 515, Loss 77.31548967957497, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 516, Loss 77.31425985693932, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 517, Loss 77.3126040995121, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 518, Loss 77.32258805632591, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 519, Loss 77.31300720572472, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 520, Loss 77.3192917406559, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 521, Loss 77.3191594183445, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 522, Loss 77.31621590256691, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 523, Loss 77.31992790102959, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 524, Loss 77.31213542819023, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 525, Loss 77.31409093737602, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 526, Loss 77.30726265907288, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 527, Loss 77.30730727314949, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 528, Loss 77.3213279247284, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 529, Loss 77.30249261856079, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 530, Loss 77.29850110411644, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 531, Loss 77.30736607313156, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 532, Loss 77.30549862980843, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 533, Loss 77.30459076166153, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 534, Loss 77.3101966381073, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 535, Loss 77.30792763829231, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 536, Loss 77.31032919883728, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 537, Loss 77.30398067831993, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 538, Loss 77.30519089102745, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 539, Loss 77.31212961673737, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 540, Loss 77.30532136559486, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 541, Loss 77.30903324484825, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 542, Loss 77.31413999199867, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 543, Loss 77.30583867430687, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 544, Loss 77.31332379579544, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 545, Loss 77.31217157840729, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 546, Loss 77.30805578827858, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 547, Loss 77.30498376488686, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 548, Loss 77.29860010743141, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 549, Loss 77.31906536221504, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 550, Loss 77.30541476607323, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 551, Loss 77.2594143152237, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 552, Loss 77.3168780207634, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 553, Loss 77.30586123466492, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 554, Loss 77.30769962072372, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 555, Loss 77.3013048171997, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 556, Loss 77.28249803185463, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 557, Loss 77.31901639699936, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 558, Loss 77.29532968997955, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 559, Loss 77.31450569629669, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 560, Loss 77.30136081576347, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 561, Loss 77.2843616604805, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 562, Loss 77.29813551902771, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 563, Loss 77.30767369270325, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 564, Loss 77.29399174451828, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 565, Loss 77.31067198514938, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 566, Loss 77.2974759042263, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 567, Loss 77.29565685987473, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 568, Loss 77.3036747276783, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 569, Loss 77.29528748989105, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 570, Loss 77.29135340452194, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 571, Loss 77.28750324249268, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 572, Loss 77.29951044917107, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 573, Loss 77.29651015996933, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 574, Loss 77.29128536581993, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 575, Loss 77.29238042235374, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 576, Loss 77.3072258234024, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 577, Loss 77.29316037893295, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 578, Loss 77.2956973016262, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 579, Loss 77.2937484383583, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 580, Loss 77.29516923427582, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 581, Loss 77.29803159832954, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 582, Loss 77.29687613248825, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 583, Loss 77.29530507326126, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 584, Loss 77.29566872119904, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 585, Loss 77.29850035905838, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 586, Loss 77.29459097981453, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 587, Loss 77.29167041182518, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 588, Loss 77.30160656571388, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 589, Loss 77.28905990719795, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 590, Loss 77.30342146754265, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 591, Loss 77.29762488603592, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 592, Loss 77.29248979687691, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 593, Loss 77.29893198609352, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 594, Loss 77.29524651169777, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 595, Loss 77.29269507527351, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 596, Loss 77.2877205312252, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 597, Loss 77.29670384526253, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 598, Loss 77.27660262584686, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 599, Loss 77.29764649271965, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 600, Loss 77.27927756309509, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 601, Loss 77.29663726687431, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 602, Loss 77.29656714200974, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 603, Loss 77.30392974615097, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 604, Loss 77.29763409495354, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 605, Loss 77.29951077699661, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 606, Loss 77.2922212779522, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 607, Loss 77.29271465539932, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 608, Loss 77.29439488053322, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 609, Loss 77.28894519805908, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 610, Loss 77.28483721613884, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 611, Loss 77.28852832317352, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 612, Loss 77.29294103384018, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 613, Loss 77.2908861041069, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 614, Loss 77.29698193073273, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 615, Loss 77.29221564531326, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 616, Loss 77.28726297616959, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 617, Loss 77.29719752073288, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 618, Loss 77.28352969884872, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 619, Loss 77.29324674606323, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 620, Loss 77.29102343320847, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 621, Loss 77.29036405682564, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 622, Loss 77.2813692688942, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 623, Loss 77.28994217514992, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 624, Loss 77.29558250308037, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 625, Loss 77.28855746984482, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 626, Loss 77.29134911298752, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 627, Loss 77.28950256109238, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 628, Loss 77.28754669427872, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 629, Loss 77.28833258152008, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 630, Loss 77.24071133136749, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 631, Loss 77.30097371339798, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 632, Loss 77.28451892733574, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 633, Loss 77.29286578297615, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 634, Loss 77.28400009870529, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 635, Loss 77.29071870446205, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 636, Loss 77.28818732500076, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 637, Loss 77.29442459344864, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 638, Loss 77.2876923084259, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 639, Loss 77.28350749611855, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 640, Loss 77.28868454694748, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 641, Loss 77.29424974322319, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 642, Loss 77.28810235857964, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 643, Loss 77.2935740351677, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 644, Loss 77.28991106152534, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 645, Loss 77.29482826590538, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 646, Loss 77.28492045402527, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 647, Loss 77.29061341285706, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 648, Loss 77.2856878042221, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 649, Loss 77.29184314608574, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 650, Loss 77.2713592350483, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 651, Loss 77.2846590578556, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 652, Loss 77.29063606262207, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 653, Loss 77.28845393657684, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 654, Loss 77.28140223026276, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 655, Loss 77.2815789282322, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 656, Loss 77.28752502799034, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 657, Loss 77.28565433621407, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 658, Loss 77.27915292978287, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 659, Loss 77.27089831233025, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 660, Loss 77.28699654340744, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 661, Loss 77.27906492352486, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 662, Loss 77.27696308493614, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 663, Loss 77.27687820792198, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 664, Loss 77.28969976305962, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 665, Loss 77.28989273309708, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 666, Loss 77.26165863871574, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 667, Loss 77.28920379281044, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 668, Loss 77.28265169262886, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 669, Loss 77.27958863973618, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 670, Loss 77.2775681912899, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 671, Loss 77.27610546350479, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 672, Loss 77.28656154870987, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 673, Loss 77.27661553025246, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 674, Loss 77.28414571285248, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 675, Loss 77.26564970612526, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 676, Loss 77.29440051317215, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 677, Loss 77.28664702177048, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 678, Loss 77.28134608268738, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 679, Loss 77.27802568674088, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 680, Loss 77.28613752126694, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 681, Loss 77.28151801228523, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 682, Loss 77.28169903159142, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 683, Loss 77.28248426318169, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 684, Loss 77.29115942120552, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 685, Loss 77.26487043499947, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 686, Loss 77.29214087128639, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 687, Loss 77.28488102555275, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 688, Loss 77.28148454427719, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 689, Loss 77.28525906801224, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 690, Loss 77.27608555555344, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 691, Loss 77.28014323115349, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 692, Loss 77.28686273097992, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 693, Loss 77.27800431847572, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 694, Loss 77.291005641222, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 695, Loss 77.27757626771927, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 696, Loss 77.28828620910645, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 697, Loss 77.27530586719513, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 698, Loss 77.28054648637772, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 699, Loss 77.28606015443802, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 700, Loss 77.28341317176819, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 701, Loss 77.28502824902534, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 702, Loss 77.285369515419, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 703, Loss 77.27675968408585, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 704, Loss 77.28441971540451, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 705, Loss 77.2811376452446, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 706, Loss 77.2792979478836, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 707, Loss 77.28361251950264, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 708, Loss 77.27914947271347, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 709, Loss 77.28933948278427, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 710, Loss 77.28000536561012, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 711, Loss 77.2752668261528, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 712, Loss 77.2808627486229, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 713, Loss 77.28154009580612, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 714, Loss 77.287338078022, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 715, Loss 77.283457249403, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 716, Loss 77.27507919073105, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 717, Loss 77.28368747234344, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 718, Loss 77.28715893626213, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 719, Loss 77.27639675140381, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 720, Loss 77.27889293432236, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 721, Loss 77.26707965135574, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 722, Loss 77.27311390638351, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 723, Loss 77.28882071375847, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 724, Loss 77.28211817145348, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 725, Loss 77.28263393044472, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 726, Loss 77.27371859550476, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 727, Loss 77.28031542897224, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 728, Loss 77.28430458903313, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 729, Loss 77.26299947500229, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 730, Loss 77.28235164284706, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 731, Loss 77.28529641032219, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 732, Loss 77.2718400657177, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 733, Loss 77.28153657913208, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 734, Loss 77.27438426017761, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 735, Loss 77.28118953108788, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 736, Loss 77.28397208452225, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 737, Loss 77.26978114247322, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 738, Loss 77.28152477741241, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 739, Loss 77.28096768260002, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 740, Loss 77.27421173453331, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 741, Loss 77.27924382686615, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 742, Loss 77.21539410948753, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 743, Loss 77.29713904857635, Train acc 0.7512842465753424, Test acc 0.7482876712328768\n",
      "Epoch 744, Loss 77.28633818030357, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 745, Loss 77.27194958925247, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 746, Loss 77.28378465771675, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 747, Loss 77.27894431352615, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 748, Loss 77.27450200915337, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 749, Loss 77.27738440036774, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 750, Loss 77.27705946564674, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 751, Loss 77.27813762426376, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 752, Loss 77.27602097392082, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 753, Loss 77.27350983023643, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 754, Loss 77.27566993236542, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 755, Loss 77.27500420808792, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 756, Loss 77.28120845556259, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 757, Loss 77.27694791555405, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 758, Loss 77.27985107898712, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 759, Loss 77.27254658937454, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 760, Loss 77.26818570494652, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 761, Loss 77.28405058383942, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 762, Loss 77.2804534137249, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 763, Loss 77.2767203450203, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 764, Loss 77.27899035811424, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 765, Loss 77.26218459010124, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 766, Loss 77.27375757694244, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 767, Loss 77.2780125439167, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 768, Loss 77.25896361470222, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 769, Loss 77.28243100643158, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 770, Loss 77.28129544854164, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 771, Loss 77.27687880396843, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 772, Loss 77.2742812037468, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 773, Loss 77.27761155366898, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 774, Loss 77.25674319267273, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 775, Loss 77.28160065412521, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 776, Loss 77.2833197414875, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 777, Loss 77.27960526943207, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 778, Loss 77.2648392021656, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 779, Loss 77.28035399317741, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 780, Loss 77.27020326256752, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 781, Loss 77.2730063199997, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 782, Loss 77.26072397828102, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 783, Loss 77.27911698818207, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 784, Loss 77.281249076128, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 785, Loss 77.26596078276634, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 786, Loss 77.29063767194748, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 787, Loss 77.27656090259552, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 788, Loss 77.26479014754295, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 789, Loss 77.27285647392273, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 790, Loss 77.28261488676071, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 791, Loss 77.2771175801754, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 792, Loss 77.27761781215668, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 793, Loss 77.27553555369377, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 794, Loss 77.27001729607582, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 795, Loss 77.28386950492859, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 796, Loss 77.27632933855057, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 797, Loss 77.28219547867775, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 798, Loss 77.27275890111923, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 799, Loss 77.27434054017067, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 800, Loss 77.27402809262276, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 801, Loss 77.27737799286842, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 802, Loss 77.28195956349373, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 803, Loss 77.27453517913818, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 804, Loss 77.28455436229706, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 805, Loss 77.28189295530319, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 806, Loss 77.26829120516777, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 807, Loss 77.27227795124054, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 808, Loss 77.27672493457794, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 809, Loss 77.27748957276344, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 810, Loss 77.27574220299721, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 811, Loss 77.22773814201355, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 812, Loss 77.28156587481499, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 813, Loss 77.27908313274384, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 814, Loss 77.27411210536957, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 815, Loss 77.2626676261425, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 816, Loss 77.26957541704178, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 817, Loss 77.28622937202454, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 818, Loss 77.2708067893982, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 819, Loss 77.27899050712585, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 820, Loss 77.27182039618492, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 821, Loss 77.25302174687386, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 822, Loss 77.28409790992737, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 823, Loss 77.27698817849159, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 824, Loss 77.26655632257462, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 825, Loss 77.27023473381996, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 826, Loss 77.2756013572216, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 827, Loss 77.27955028414726, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 828, Loss 77.27597481012344, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 829, Loss 77.27847161889076, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 830, Loss 77.27103224396706, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 831, Loss 77.27468261122704, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 832, Loss 77.2704508304596, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 833, Loss 77.27641239762306, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 834, Loss 77.27723371982574, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 835, Loss 77.27885690331459, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 836, Loss 77.27132520079613, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 837, Loss 77.27635663747787, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 838, Loss 77.26723054051399, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 839, Loss 77.27952167391777, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 840, Loss 77.28361287713051, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 841, Loss 77.27746433019638, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 842, Loss 77.27415010333061, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 843, Loss 77.2593649327755, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 844, Loss 77.2618642449379, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 845, Loss 77.28851002454758, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 846, Loss 77.2675573527813, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 847, Loss 77.27425599098206, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 848, Loss 77.27517119050026, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 849, Loss 77.25144764780998, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 850, Loss 77.27766042947769, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 851, Loss 77.26457649469376, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 852, Loss 77.25419569015503, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 853, Loss 77.27646455168724, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 854, Loss 77.27259463071823, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 855, Loss 77.27288308739662, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 856, Loss 77.27340486645699, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 857, Loss 77.27793005108833, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 858, Loss 77.27195331454277, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 859, Loss 77.27388447523117, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 860, Loss 77.2759045958519, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 861, Loss 77.27530086040497, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 862, Loss 77.27412912249565, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 863, Loss 77.27436792850494, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 864, Loss 77.26545515656471, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 865, Loss 77.26456066966057, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 866, Loss 77.27931427955627, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 867, Loss 77.28277572989464, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 868, Loss 77.26293587684631, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 869, Loss 77.26454171538353, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 870, Loss 77.27882048487663, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 871, Loss 77.2674169242382, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 872, Loss 77.25492319464684, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 873, Loss 77.27511668205261, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 874, Loss 77.2801324725151, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 875, Loss 77.27533710002899, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 876, Loss 77.27062159776688, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 877, Loss 77.26988190412521, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 878, Loss 77.25961190462112, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 879, Loss 77.28477120399475, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 880, Loss 77.2691969871521, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 881, Loss 77.26883974671364, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 882, Loss 77.28010511398315, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 883, Loss 77.2734349668026, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 884, Loss 77.26209458708763, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 885, Loss 77.25826823711395, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 886, Loss 77.28091019392014, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 887, Loss 77.26289960741997, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 888, Loss 77.27031323313713, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 889, Loss 77.26541832089424, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 890, Loss 77.25834083557129, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 891, Loss 77.26678311824799, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 892, Loss 77.25669220089912, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 893, Loss 77.28036624193192, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 894, Loss 77.26452189683914, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 895, Loss 77.26284927129745, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 896, Loss 77.27571976184845, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 897, Loss 77.2696450650692, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 898, Loss 77.27363327145576, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 899, Loss 77.27171459794044, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 900, Loss 77.26081040501595, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 901, Loss 77.28929063677788, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 902, Loss 77.2741202712059, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 903, Loss 77.26185849308968, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 904, Loss 77.27469182014465, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 905, Loss 77.27290913462639, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 906, Loss 77.26783809065819, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 907, Loss 77.26966002583504, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 908, Loss 77.2787261903286, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 909, Loss 77.26437345147133, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 910, Loss 77.27559703588486, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 911, Loss 77.2637428343296, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 912, Loss 77.27316898107529, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 913, Loss 77.27749374508858, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 914, Loss 77.27841183543205, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 915, Loss 77.26771253347397, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 916, Loss 77.27271097898483, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 917, Loss 77.26281622052193, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 918, Loss 77.25885513424873, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 919, Loss 77.27375862002373, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 920, Loss 77.27687898278236, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 921, Loss 77.27744579315186, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 922, Loss 77.26446878910065, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 923, Loss 77.27260863780975, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 924, Loss 77.27001467347145, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 925, Loss 77.25980722904205, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 926, Loss 77.28409445285797, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 927, Loss 77.28342840075493, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 928, Loss 77.27110007405281, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 929, Loss 77.25927758216858, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 930, Loss 77.27842873334885, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 931, Loss 77.26489022374153, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 932, Loss 77.26020431518555, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 933, Loss 77.27715796232224, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 934, Loss 77.27063035964966, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 935, Loss 77.27117970585823, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 936, Loss 77.265227496624, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 937, Loss 77.26842296123505, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 938, Loss 77.27168422937393, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 939, Loss 77.26701667904854, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 940, Loss 77.25779283046722, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 941, Loss 77.21018743515015, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 942, Loss 77.28005796670914, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 943, Loss 77.27391284704208, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 944, Loss 77.2694807946682, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 945, Loss 77.26845607161522, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 946, Loss 77.26375052332878, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 947, Loss 77.26523220539093, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 948, Loss 77.27729868888855, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 949, Loss 77.2669412791729, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 950, Loss 77.27729067206383, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 951, Loss 77.26999816298485, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 952, Loss 77.26980516314507, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 953, Loss 77.26755338907242, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 954, Loss 77.27913129329681, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 955, Loss 77.27675387263298, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 956, Loss 77.26970344781876, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 957, Loss 77.23642754554749, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 958, Loss 77.27949517965317, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 959, Loss 77.26621118187904, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 960, Loss 77.2473932504654, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 961, Loss 77.27154043316841, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 962, Loss 77.27428069710732, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 963, Loss 77.26991021633148, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 964, Loss 77.27262967824936, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 965, Loss 77.27167558670044, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 966, Loss 77.27510422468185, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 967, Loss 77.25503805279732, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 968, Loss 77.27385959029198, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 969, Loss 77.26757177710533, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 970, Loss 77.27314218878746, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 971, Loss 77.27813509106636, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 972, Loss 77.26867786049843, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 973, Loss 77.27445182204247, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 974, Loss 77.26505270600319, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 975, Loss 77.27233150601387, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 976, Loss 77.26835343241692, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 977, Loss 77.27192807197571, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 978, Loss 77.28090143203735, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 979, Loss 77.25834465026855, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 980, Loss 77.27197510004044, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 981, Loss 77.25908461213112, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 982, Loss 77.26050239801407, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 983, Loss 77.27093514800072, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 984, Loss 77.25502762198448, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 985, Loss 77.27149140834808, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 986, Loss 77.27100875973701, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 987, Loss 77.27620953321457, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 988, Loss 77.26601874828339, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 989, Loss 77.26399266719818, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 990, Loss 77.26717072725296, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 991, Loss 77.27266937494278, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 992, Loss 77.25390276312828, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 993, Loss 77.27337917685509, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 994, Loss 77.26565742492676, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 995, Loss 77.2657240331173, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 996, Loss 77.27415096759796, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 997, Loss 77.26063060760498, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 998, Loss 77.27215653657913, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 999, Loss 77.27154210209846, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1000, Loss 77.27113860845566, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1001, Loss 77.26974710822105, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1002, Loss 77.2648951113224, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1003, Loss 77.2747372686863, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1004, Loss 77.27365580201149, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1005, Loss 77.25403019785881, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1006, Loss 77.27223813533783, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1007, Loss 77.27470257878304, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1008, Loss 77.274469435215, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1009, Loss 77.26591819524765, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1010, Loss 77.27774477005005, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1011, Loss 77.26717874407768, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1012, Loss 77.27269411087036, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1013, Loss 77.27116423845291, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1014, Loss 77.26599407196045, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1015, Loss 77.26967903971672, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1016, Loss 77.27063828706741, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1017, Loss 77.2601862847805, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1018, Loss 77.26525032520294, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1019, Loss 77.27869907021523, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1020, Loss 77.27543839812279, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1021, Loss 77.26688224077225, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1022, Loss 77.27420118451118, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1023, Loss 77.267733335495, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1024, Loss 77.26797139644623, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1025, Loss 77.27735900878906, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1026, Loss 77.27483242750168, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1027, Loss 77.26916688680649, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1028, Loss 77.26978787779808, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1029, Loss 77.2640483379364, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1030, Loss 77.26995050907135, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1031, Loss 77.26539698243141, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1032, Loss 77.26999652385712, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1033, Loss 77.26927733421326, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1034, Loss 77.2774572968483, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1035, Loss 77.26527634263039, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1036, Loss 77.27302441000938, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1037, Loss 77.26750138401985, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1038, Loss 77.25654724240303, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1039, Loss 77.26086366176605, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1040, Loss 77.2683761715889, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1041, Loss 77.26818385720253, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1042, Loss 77.26780864596367, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1043, Loss 77.26755547523499, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1044, Loss 77.27137500047684, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1045, Loss 77.27161890268326, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1046, Loss 77.27685740590096, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1047, Loss 77.26568341255188, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1048, Loss 77.27161306142807, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1049, Loss 77.26614111661911, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1050, Loss 77.22878283262253, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1051, Loss 77.27231821417809, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 1052, Loss 77.26668056845665, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1053, Loss 77.26630082726479, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1054, Loss 77.26574155688286, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1055, Loss 77.26556411385536, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1056, Loss 77.27720203995705, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1057, Loss 77.26125052571297, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1058, Loss 77.24733912944794, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1059, Loss 77.27372166514397, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 1060, Loss 77.2696838080883, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1061, Loss 77.23280763626099, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1062, Loss 77.26736751198769, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 1063, Loss 77.26961836218834, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1064, Loss 77.27106791734695, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1065, Loss 77.2664549946785, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1066, Loss 77.26957184076309, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1067, Loss 77.26506105065346, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1068, Loss 77.27373310923576, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1069, Loss 77.26850509643555, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1070, Loss 77.26542735099792, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1071, Loss 77.27460339665413, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1072, Loss 77.26731386780739, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1073, Loss 77.2647856771946, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1074, Loss 77.26493182778358, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1075, Loss 77.26962634921074, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1076, Loss 77.27429845929146, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1077, Loss 77.2746929526329, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1078, Loss 77.27594548463821, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1079, Loss 77.26539519429207, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1080, Loss 77.26610577106476, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1081, Loss 77.2664086818695, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1082, Loss 77.27027809619904, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1083, Loss 77.27384603023529, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1084, Loss 77.26723390817642, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1085, Loss 77.2669329047203, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1086, Loss 77.26874095201492, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1087, Loss 77.26535719633102, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1088, Loss 77.26916697621346, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1089, Loss 77.27358257770538, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1090, Loss 77.26493668556213, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1091, Loss 77.26079282164574, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1092, Loss 77.25557300448418, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1093, Loss 77.27728456258774, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 1094, Loss 77.2702848315239, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1095, Loss 77.24627894163132, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1096, Loss 77.28174015879631, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 1097, Loss 77.26792308688164, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1098, Loss 77.25816398859024, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1099, Loss 77.27241840958595, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 1100, Loss 77.27675607800484, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1101, Loss 77.2745746076107, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1102, Loss 77.27221924066544, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1103, Loss 77.27309289574623, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1104, Loss 77.26411411166191, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1105, Loss 77.26707339286804, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1106, Loss 77.27174428105354, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1107, Loss 77.26760897040367, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1108, Loss 77.264384329319, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1109, Loss 77.27245560288429, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1110, Loss 77.26821705698967, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1111, Loss 77.26427182555199, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1112, Loss 77.27160140872002, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1113, Loss 77.26897704601288, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1114, Loss 77.27052533626556, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1115, Loss 77.26595768332481, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1116, Loss 77.270817309618, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1117, Loss 77.27431398630142, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1118, Loss 77.27108356356621, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1119, Loss 77.25474587082863, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1120, Loss 77.27151796221733, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 1121, Loss 77.25852152705193, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1122, Loss 77.26391917467117, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1123, Loss 77.25333398580551, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1124, Loss 77.277203977108, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 1125, Loss 77.26769223809242, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1126, Loss 77.26645219326019, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1127, Loss 77.26846659183502, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1128, Loss 77.24429279565811, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1129, Loss 77.27615308761597, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1130, Loss 77.27156910300255, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1131, Loss 77.27198266983032, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1132, Loss 77.26223328709602, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1133, Loss 77.26438906788826, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1134, Loss 77.26877927780151, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1135, Loss 77.26649314165115, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1136, Loss 77.26863861083984, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1137, Loss 77.25984492897987, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1138, Loss 77.26956003904343, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1139, Loss 77.25710958242416, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1140, Loss 77.26875883340836, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1141, Loss 77.27229860424995, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1142, Loss 77.26868292689323, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1143, Loss 77.2526228427887, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1144, Loss 77.26864358782768, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1145, Loss 77.26857796311378, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1146, Loss 77.2727675139904, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1147, Loss 77.27031683921814, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1148, Loss 77.27280673384666, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1149, Loss 77.26745572686195, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1150, Loss 77.26560559868813, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1151, Loss 77.26190620660782, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1152, Loss 77.26494178175926, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1153, Loss 77.25818172097206, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1154, Loss 77.27280515432358, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1155, Loss 77.26577949523926, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1156, Loss 77.27141857147217, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1157, Loss 77.26930117607117, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1158, Loss 77.26538941264153, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1159, Loss 77.26197245717049, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1160, Loss 77.26727390289307, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1161, Loss 77.26803830265999, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1162, Loss 77.2650237083435, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1163, Loss 77.27550867199898, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1164, Loss 77.26386123895645, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1165, Loss 77.26363849639893, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1166, Loss 77.26318684220314, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1167, Loss 77.26026380062103, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1168, Loss 77.27050840854645, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1169, Loss 77.2688561975956, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1170, Loss 77.26896592974663, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1171, Loss 77.26556226611137, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1172, Loss 77.27017602324486, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1173, Loss 77.26348805427551, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1174, Loss 77.27393451333046, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1175, Loss 77.27240279316902, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1176, Loss 77.26646474003792, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1177, Loss 77.26359182596207, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1178, Loss 77.26206478476524, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1179, Loss 77.26657423377037, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1180, Loss 77.26765650510788, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1181, Loss 77.26927745342255, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1182, Loss 77.26449581980705, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1183, Loss 77.26819723844528, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1184, Loss 77.26776027679443, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1185, Loss 77.26405328512192, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1186, Loss 77.25864100456238, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1187, Loss 77.26970908045769, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1188, Loss 77.27187278866768, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1189, Loss 77.2639948129654, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1190, Loss 77.27928957343102, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 1191, Loss 77.26401641964912, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1192, Loss 77.2666649222374, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1193, Loss 77.26859176158905, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1194, Loss 77.26334688067436, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1195, Loss 77.26514655351639, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1196, Loss 77.26658064126968, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1197, Loss 77.26559245586395, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1198, Loss 77.26641124486923, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1199, Loss 77.27137795090675, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1200, Loss 77.26423490047455, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1201, Loss 77.26199501752853, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1202, Loss 77.26689139008522, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1203, Loss 77.2606951892376, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1204, Loss 77.26373532414436, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1205, Loss 77.27701985836029, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1206, Loss 77.24045452475548, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1207, Loss 77.27578926086426, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 1208, Loss 77.26527500152588, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1209, Loss 77.26036524772644, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1210, Loss 77.26800820231438, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1211, Loss 77.27254527807236, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1212, Loss 77.26775792241096, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1213, Loss 77.26111450791359, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1214, Loss 77.26723313331604, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1215, Loss 77.26653796434402, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1216, Loss 77.2664832174778, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1217, Loss 77.27036276459694, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1218, Loss 77.27589926123619, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1219, Loss 77.25305613875389, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1220, Loss 77.26834404468536, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1221, Loss 77.2696533203125, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1222, Loss 77.25679197907448, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1223, Loss 77.26345708966255, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1224, Loss 77.26319259405136, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1225, Loss 77.26238569617271, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1226, Loss 77.26336577534676, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1227, Loss 77.26171237230301, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1228, Loss 77.2633590400219, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1229, Loss 77.26658830046654, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1230, Loss 77.26497587561607, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1231, Loss 77.27484166622162, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1232, Loss 77.2622062265873, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1233, Loss 77.26704281568527, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1234, Loss 77.26687777042389, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1235, Loss 77.2492161989212, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1236, Loss 77.27059412002563, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 1237, Loss 77.26069468259811, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1238, Loss 77.26382759213448, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1239, Loss 77.26013469696045, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1240, Loss 77.26889967918396, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1241, Loss 77.26261860132217, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1242, Loss 77.2549138367176, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1243, Loss 77.2713428735733, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1244, Loss 77.24226576089859, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1245, Loss 77.26922661066055, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1246, Loss 77.25792679190636, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1247, Loss 77.26412063837051, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1248, Loss 77.26633280515671, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1249, Loss 77.2653893828392, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1250, Loss 77.26167914271355, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1251, Loss 77.26353561878204, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1252, Loss 77.24765902757645, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1253, Loss 77.26805526018143, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1254, Loss 77.267956584692, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1255, Loss 77.26760673522949, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1256, Loss 77.261893928051, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1257, Loss 77.2704750597477, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1258, Loss 77.2695603966713, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1259, Loss 77.25451973080635, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1260, Loss 77.26717299222946, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1261, Loss 77.26605346798897, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1262, Loss 77.2660581767559, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1263, Loss 77.27015382051468, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1264, Loss 77.26499384641647, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1265, Loss 77.27167424559593, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1266, Loss 77.27275401353836, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1267, Loss 77.26731288433075, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1268, Loss 77.27829346060753, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1269, Loss 77.25307112932205, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1270, Loss 77.270090341568, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1271, Loss 77.27086725831032, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1272, Loss 77.26274192333221, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1273, Loss 77.26824241876602, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1274, Loss 77.2634496986866, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1275, Loss 77.26842606067657, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1276, Loss 77.2616101205349, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1277, Loss 77.2708512544632, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1278, Loss 77.26674515008926, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1279, Loss 77.26717665791512, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1280, Loss 77.26755863428116, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1281, Loss 77.26742500066757, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1282, Loss 77.26248273253441, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1283, Loss 77.26952016353607, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1284, Loss 77.26819303631783, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1285, Loss 77.2606161236763, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1286, Loss 77.27091723680496, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1287, Loss 77.26818466186523, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1288, Loss 77.26481699943542, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1289, Loss 77.26528218388557, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1290, Loss 77.26475650072098, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1291, Loss 77.26214683055878, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1292, Loss 77.26733496785164, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1293, Loss 77.26456007361412, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1294, Loss 77.26363211870193, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1295, Loss 77.26070147752762, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1296, Loss 77.26056158542633, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1297, Loss 77.26103469729424, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1298, Loss 77.26731818914413, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1299, Loss 77.2666015625, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1300, Loss 77.26652273535728, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1301, Loss 77.26983076334, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1302, Loss 77.26358544826508, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1303, Loss 77.2513398528099, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1304, Loss 77.2750868499279, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1305, Loss 77.26054695248604, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1306, Loss 77.2675676047802, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1307, Loss 77.26042932271957, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1308, Loss 77.27384713292122, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1309, Loss 77.25907066464424, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1310, Loss 77.2676816880703, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1311, Loss 77.26106563210487, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1312, Loss 77.27039381861687, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1313, Loss 77.26252833008766, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1314, Loss 77.27238842844963, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1315, Loss 77.26373863220215, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1316, Loss 77.2550351023674, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1317, Loss 77.26551133394241, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1318, Loss 77.26725473999977, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1319, Loss 77.26440972089767, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1320, Loss 77.26144433021545, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1321, Loss 77.26778683066368, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1322, Loss 77.25407323241234, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1323, Loss 77.26744785904884, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1324, Loss 77.26626980304718, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1325, Loss 77.26711714267731, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1326, Loss 77.26350179314613, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1327, Loss 77.26163229346275, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1328, Loss 77.26715677976608, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1329, Loss 77.26649191975594, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1330, Loss 77.26001244783401, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1331, Loss 77.26843637228012, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1332, Loss 77.26253911852837, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1333, Loss 77.26525789499283, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1334, Loss 77.26058277487755, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1335, Loss 77.2614877820015, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1336, Loss 77.26245507597923, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1337, Loss 77.26896041631699, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1338, Loss 77.26729601621628, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1339, Loss 77.26740807294846, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1340, Loss 77.26929986476898, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1341, Loss 77.27444046735764, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1342, Loss 77.2650778889656, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1343, Loss 77.26579755544662, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1344, Loss 77.25932508707047, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1345, Loss 77.26621615886688, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1346, Loss 77.26461309194565, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1347, Loss 77.26762375235558, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1348, Loss 77.26355564594269, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1349, Loss 77.26181504130363, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1350, Loss 77.26420876383781, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1351, Loss 77.26187679171562, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1352, Loss 77.26861488819122, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1353, Loss 77.2645181119442, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1354, Loss 77.26016402244568, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1355, Loss 77.26106345653534, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1356, Loss 77.2657559812069, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1357, Loss 77.26932182908058, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1358, Loss 77.26597082614899, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1359, Loss 77.26751896739006, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1360, Loss 77.26169884204865, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1361, Loss 77.26797431707382, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1362, Loss 77.26471608877182, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1363, Loss 77.26532620191574, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1364, Loss 77.26848495006561, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1365, Loss 77.26668593287468, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1366, Loss 77.26776969432831, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1367, Loss 77.2653748691082, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1368, Loss 77.25862663984299, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1369, Loss 77.2603830397129, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1370, Loss 77.26269575953484, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1371, Loss 77.26574665307999, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1372, Loss 77.27071934938431, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1373, Loss 77.26679956912994, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1374, Loss 77.26339563727379, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1375, Loss 77.26611667871475, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1376, Loss 77.26560401916504, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1377, Loss 77.26844298839569, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1378, Loss 77.2709998190403, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1379, Loss 77.26856249570847, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1380, Loss 77.26906934380531, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1381, Loss 77.26125463843346, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1382, Loss 77.26707157492638, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1383, Loss 77.26988184452057, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1384, Loss 77.24954006075859, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1385, Loss 77.26313245296478, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1386, Loss 77.26467761397362, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1387, Loss 77.26727119088173, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1388, Loss 77.25328123569489, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1389, Loss 77.2558718919754, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1390, Loss 77.26648232340813, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1391, Loss 77.26757282018661, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1392, Loss 77.26345840096474, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1393, Loss 77.26095706224442, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1394, Loss 77.26609426736832, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1395, Loss 77.27019956707954, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1396, Loss 77.25819233059883, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1397, Loss 77.26391419768333, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1398, Loss 77.26490494608879, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1399, Loss 77.26718956232071, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1400, Loss 77.26527997851372, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1401, Loss 77.25521942973137, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1402, Loss 77.26248240470886, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1403, Loss 77.26524326205254, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1404, Loss 77.26686033606529, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1405, Loss 77.26070415973663, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1406, Loss 77.27341750264168, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1407, Loss 77.26184886693954, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1408, Loss 77.26123860478401, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1409, Loss 77.26608085632324, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1410, Loss 77.26583647727966, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1411, Loss 77.26297080516815, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1412, Loss 77.26447466015816, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1413, Loss 77.26766353845596, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1414, Loss 77.26801803708076, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1415, Loss 77.25815460085869, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1416, Loss 77.25212079286575, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1417, Loss 77.27010518312454, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1418, Loss 77.2669489979744, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1419, Loss 77.26858949661255, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1420, Loss 77.2662633061409, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1421, Loss 77.2656572163105, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1422, Loss 77.26738765835762, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1423, Loss 77.26145586371422, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1424, Loss 77.26640537381172, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1425, Loss 77.26064094901085, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1426, Loss 77.27082285284996, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1427, Loss 77.26262855529785, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1428, Loss 77.26051831245422, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1429, Loss 77.26130416989326, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1430, Loss 77.26467707753181, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1431, Loss 77.26362213492393, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1432, Loss 77.26552161574364, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1433, Loss 77.26925650238991, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1434, Loss 77.26930612325668, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1435, Loss 77.2647790312767, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1436, Loss 77.25497898459435, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1437, Loss 77.26000413298607, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1438, Loss 77.25370141863823, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1439, Loss 77.26616287231445, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1440, Loss 77.26437029242516, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1441, Loss 77.26534205675125, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1442, Loss 77.26149263978004, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1443, Loss 77.26431825757027, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1444, Loss 77.25463265180588, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1445, Loss 77.27137124538422, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1446, Loss 77.26234272122383, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1447, Loss 77.26488015055656, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1448, Loss 77.26417654752731, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1449, Loss 77.26418268680573, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1450, Loss 77.26776292920113, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1451, Loss 77.2589638531208, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1452, Loss 77.26374471187592, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1453, Loss 77.26313644647598, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1454, Loss 77.26438194513321, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1455, Loss 77.26800134778023, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1456, Loss 77.25344577431679, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1457, Loss 77.27045187354088, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1458, Loss 77.2623440027237, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1459, Loss 77.264443308115, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1460, Loss 77.23329073190689, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1461, Loss 77.27207478880882, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 1462, Loss 77.26326757669449, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1463, Loss 77.26386204361916, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1464, Loss 77.26380130648613, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1465, Loss 77.25628134608269, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1466, Loss 77.26637336611748, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1467, Loss 77.24063393473625, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1468, Loss 77.26666340231895, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 1469, Loss 77.26281586289406, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1470, Loss 77.26920711994171, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1471, Loss 77.25139421224594, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1472, Loss 77.26109376549721, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1473, Loss 77.26820221543312, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1474, Loss 77.25901979207993, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1475, Loss 77.27368378639221, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1476, Loss 77.26374697685242, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1477, Loss 77.26561632752419, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1478, Loss 77.26419684290886, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1479, Loss 77.26867565512657, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1480, Loss 77.26855543255806, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1481, Loss 77.26689231395721, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1482, Loss 77.26716455817223, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1483, Loss 77.26374313235283, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1484, Loss 77.2625080049038, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1485, Loss 77.26218640804291, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1486, Loss 77.26799234747887, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1487, Loss 77.26677018404007, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1488, Loss 77.2559829056263, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1489, Loss 77.25838023424149, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1490, Loss 77.26089286804199, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1491, Loss 77.2657421529293, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1492, Loss 77.26049852371216, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1493, Loss 77.25933027267456, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1494, Loss 77.26068103313446, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1495, Loss 77.26417651772499, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1496, Loss 77.26139360666275, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1497, Loss 77.26886388659477, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1498, Loss 77.26791548728943, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1499, Loss 77.26444011926651, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1500, Loss 77.26632523536682, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1501, Loss 77.25942751765251, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1502, Loss 77.26617795228958, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1503, Loss 77.2652657032013, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1504, Loss 77.26020082831383, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1505, Loss 77.25770989060402, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1506, Loss 77.27208271622658, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1507, Loss 77.26086607575417, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1508, Loss 77.26268520951271, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1509, Loss 77.26887544989586, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1510, Loss 77.26444917917252, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1511, Loss 77.26794865727425, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1512, Loss 77.26074010133743, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1513, Loss 77.26551908254623, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1514, Loss 77.25959759950638, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1515, Loss 77.26588329672813, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1516, Loss 77.26745289564133, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1517, Loss 77.26434814929962, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1518, Loss 77.25814163684845, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1519, Loss 77.26592627167702, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1520, Loss 77.26066249608994, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1521, Loss 77.2584599852562, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1522, Loss 77.26852875947952, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1523, Loss 77.2691450715065, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1524, Loss 77.25868532061577, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1525, Loss 77.26431977748871, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1526, Loss 77.2569198012352, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1527, Loss 77.26829761266708, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1528, Loss 77.26916819810867, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1529, Loss 77.25782498717308, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1530, Loss 77.26063621044159, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1531, Loss 77.2717952132225, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1532, Loss 77.26294711232185, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1533, Loss 77.26675474643707, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1534, Loss 77.26131671667099, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1535, Loss 77.26610627770424, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1536, Loss 77.26604843139648, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1537, Loss 77.2663725912571, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1538, Loss 77.2624760568142, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1539, Loss 77.267409324646, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1540, Loss 77.26176649332047, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1541, Loss 77.26154485344887, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1542, Loss 77.26598709821701, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1543, Loss 77.26083716750145, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1544, Loss 77.26446080207825, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1545, Loss 77.26206681132317, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1546, Loss 77.26245820522308, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1547, Loss 77.26519277691841, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1548, Loss 77.26284232735634, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1549, Loss 77.25918197631836, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1550, Loss 77.24331739544868, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1551, Loss 77.26093500852585, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1552, Loss 77.26464259624481, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1553, Loss 77.26390653848648, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1554, Loss 77.27387458086014, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1555, Loss 77.26724392175674, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1556, Loss 77.26126062870026, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1557, Loss 77.26044982671738, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1558, Loss 77.24508330225945, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1559, Loss 77.26136645674706, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1560, Loss 77.26117065548897, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1561, Loss 77.25904351472855, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1562, Loss 77.26113820075989, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1563, Loss 77.26188516616821, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1564, Loss 77.26251196861267, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1565, Loss 77.26820743083954, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1566, Loss 77.25713437795639, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1567, Loss 77.26371750235558, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1568, Loss 77.26642286777496, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1569, Loss 77.26051607728004, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1570, Loss 77.25722146034241, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1571, Loss 77.26722574234009, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1572, Loss 77.2623559832573, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1573, Loss 77.2644034922123, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1574, Loss 77.25767883658409, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1575, Loss 77.26445087790489, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1576, Loss 77.2609024643898, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1577, Loss 77.26153939962387, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1578, Loss 77.25896149873734, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1579, Loss 77.25975579023361, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1580, Loss 77.26467752456665, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1581, Loss 77.26326489448547, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1582, Loss 77.26124507188797, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1583, Loss 77.27074983716011, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1584, Loss 77.25609993934631, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1585, Loss 77.2677900493145, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1586, Loss 77.25660839676857, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1587, Loss 77.26464205980301, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1588, Loss 77.26035901904106, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1589, Loss 77.25526043772697, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1590, Loss 77.26191195845604, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1591, Loss 77.26369133591652, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1592, Loss 77.25964152812958, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1593, Loss 77.25843897461891, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1594, Loss 77.26745262742043, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1595, Loss 77.25182351469994, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1596, Loss 77.25850972533226, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1597, Loss 77.2684121131897, Train acc 0.7502140410958904, Test acc 0.7482876712328768\n",
      "Epoch 1598, Loss 77.26256611943245, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1599, Loss 77.25544539093971, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1600, Loss 77.25692069530487, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1601, Loss 77.26148265600204, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1602, Loss 77.2678459584713, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1603, Loss 77.25331091880798, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1604, Loss 77.2681132555008, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1605, Loss 77.26667645573616, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1606, Loss 77.26566743850708, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1607, Loss 77.26948630809784, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1608, Loss 77.2553700208664, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1609, Loss 77.26146525144577, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1610, Loss 77.26733860373497, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1611, Loss 77.25757449865341, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1612, Loss 77.26662513613701, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1613, Loss 77.26362890005112, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1614, Loss 77.25939673185349, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1615, Loss 77.26605632901192, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1616, Loss 77.2628861963749, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1617, Loss 77.26356026530266, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1618, Loss 77.26667296886444, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1619, Loss 77.26336577534676, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1620, Loss 77.25975698232651, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1621, Loss 77.26896193623543, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1622, Loss 77.263049274683, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1623, Loss 77.26855221390724, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1624, Loss 77.26111620664597, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1625, Loss 77.2648176252842, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1626, Loss 77.26373413205147, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1627, Loss 77.25385820865631, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1628, Loss 77.26774245500565, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1629, Loss 77.26005017757416, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1630, Loss 77.26626726984978, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1631, Loss 77.25942543148994, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1632, Loss 77.26193055510521, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1633, Loss 77.2662398815155, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1634, Loss 77.26540216803551, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1635, Loss 77.26478728652, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1636, Loss 77.26318487524986, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1637, Loss 77.26325592398643, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1638, Loss 77.26365950703621, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1639, Loss 77.26729154586792, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1640, Loss 77.2723278105259, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1641, Loss 77.25899142026901, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1642, Loss 77.25969123840332, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1643, Loss 77.27163669466972, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1644, Loss 77.259332716465, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1645, Loss 77.26175680756569, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1646, Loss 77.26658856868744, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1647, Loss 77.26682671904564, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1648, Loss 77.26267874240875, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1649, Loss 77.26203113794327, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1650, Loss 77.26284492015839, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1651, Loss 77.26351591944695, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1652, Loss 77.25968065857887, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1653, Loss 77.26738294959068, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1654, Loss 77.26407188177109, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1655, Loss 77.2728196978569, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1656, Loss 77.26601669192314, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1657, Loss 77.25304388999939, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1658, Loss 77.25728413462639, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1659, Loss 77.265376329422, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1660, Loss 77.26202315092087, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1661, Loss 77.25997963547707, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1662, Loss 77.26667800545692, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1663, Loss 77.27177971601486, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1664, Loss 77.26033908128738, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1665, Loss 77.26515090465546, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1666, Loss 77.26285097002983, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1667, Loss 77.26523944735527, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1668, Loss 77.25412505865097, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1669, Loss 77.26426500082016, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1670, Loss 77.25646075606346, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1671, Loss 77.2543531358242, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1672, Loss 77.2672276198864, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1673, Loss 77.25853332877159, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1674, Loss 77.23987129330635, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1675, Loss 77.26584178209305, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1676, Loss 77.26822277903557, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1677, Loss 77.25564640760422, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1678, Loss 77.25502753257751, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1679, Loss 77.26080831885338, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1680, Loss 77.26228880882263, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1681, Loss 77.25630643963814, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1682, Loss 77.26773503422737, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1683, Loss 77.26400437951088, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1684, Loss 77.26335981488228, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1685, Loss 77.2579673230648, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1686, Loss 77.26309564709663, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1687, Loss 77.24974691867828, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1688, Loss 77.2631407380104, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1689, Loss 77.26044926047325, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1690, Loss 77.25699982047081, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1691, Loss 77.2643880546093, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1692, Loss 77.27035200595856, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1693, Loss 77.25967916846275, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1694, Loss 77.26218116283417, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1695, Loss 77.2634899020195, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1696, Loss 77.26422321796417, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1697, Loss 77.26202201843262, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1698, Loss 77.26180070638657, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1699, Loss 77.26095199584961, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1700, Loss 77.266266644001, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1701, Loss 77.25722950696945, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1702, Loss 77.26559698581696, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1703, Loss 77.26078867912292, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1704, Loss 77.26893872022629, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1705, Loss 77.26468795537949, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1706, Loss 77.25931519269943, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1707, Loss 77.26650011539459, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1708, Loss 77.26482248306274, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1709, Loss 77.26476195454597, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1710, Loss 77.26945942640305, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1711, Loss 77.2670793235302, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1712, Loss 77.26075345277786, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1713, Loss 77.26612192392349, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1714, Loss 77.25674772262573, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1715, Loss 77.26326507329941, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1716, Loss 77.26367768645287, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1717, Loss 77.2623461484909, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1718, Loss 77.26196604967117, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1719, Loss 77.26394918560982, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1720, Loss 77.25958532094955, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1721, Loss 77.25848141312599, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1722, Loss 77.26176807284355, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1723, Loss 77.2649986743927, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1724, Loss 77.25988230109215, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1725, Loss 77.26712462306023, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1726, Loss 77.25546252727509, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1727, Loss 77.25726193189621, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1728, Loss 77.26079949736595, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1729, Loss 77.259033203125, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1730, Loss 77.26432865858078, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1731, Loss 77.24236026406288, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1732, Loss 77.25925776362419, Train acc 0.751070205479452, Test acc 0.7482876712328768\n",
      "Epoch 1733, Loss 77.2596125304699, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1734, Loss 77.25979307293892, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1735, Loss 77.25715935230255, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1736, Loss 77.26142331957817, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1737, Loss 77.26563984155655, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1738, Loss 77.26140221953392, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1739, Loss 77.26265376806259, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1740, Loss 77.26403972506523, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1741, Loss 77.25909304618835, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1742, Loss 77.25400072336197, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1743, Loss 77.26384085416794, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1744, Loss 77.26193788647652, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1745, Loss 77.25440058112144, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1746, Loss 77.26794168353081, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1747, Loss 77.25080987811089, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1748, Loss 77.26351276040077, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1749, Loss 77.2696064710617, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1750, Loss 77.25923573970795, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1751, Loss 77.25237455964088, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1752, Loss 77.26585167646408, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1753, Loss 77.26326921582222, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1754, Loss 77.26111906766891, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1755, Loss 77.25524285435677, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1756, Loss 77.25608590245247, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1757, Loss 77.23797130584717, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1758, Loss 77.27453801035881, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1759, Loss 77.26392257213593, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1760, Loss 77.26415625214577, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1761, Loss 77.26304534077644, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1762, Loss 77.2610715329647, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1763, Loss 77.26035928726196, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1764, Loss 77.24999541044235, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1765, Loss 77.26336416602135, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1766, Loss 77.26159623265266, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1767, Loss 77.2610572874546, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1768, Loss 77.25699061155319, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1769, Loss 77.25734108686447, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1770, Loss 77.25812593102455, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1771, Loss 77.26009601354599, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1772, Loss 77.26182562112808, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1773, Loss 77.26011392474174, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1774, Loss 77.26781502366066, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1775, Loss 77.26659616827965, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1776, Loss 77.25972408056259, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1777, Loss 77.25790414214134, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1778, Loss 77.26076939702034, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1779, Loss 77.2585526406765, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1780, Loss 77.26769387722015, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1781, Loss 77.26370686292648, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1782, Loss 77.24601033329964, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1783, Loss 77.26881462335587, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1784, Loss 77.26905247569084, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1785, Loss 77.2652676999569, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1786, Loss 77.25969994068146, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1787, Loss 77.27095636725426, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1788, Loss 77.26558232307434, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1789, Loss 77.25847619771957, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1790, Loss 77.2568196952343, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1791, Loss 77.26337575912476, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1792, Loss 77.25916859507561, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1793, Loss 77.26572263240814, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1794, Loss 77.2642871439457, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1795, Loss 77.26247957348824, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1796, Loss 77.26336467266083, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1797, Loss 77.26391279697418, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1798, Loss 77.25565329194069, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1799, Loss 77.26637014746666, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1800, Loss 77.25989952683449, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1801, Loss 77.25906309485435, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1802, Loss 77.26573759317398, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1803, Loss 77.25620168447495, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1804, Loss 77.24913799762726, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1805, Loss 77.2538058757782, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1806, Loss 77.25908488035202, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1807, Loss 77.24621993303299, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1808, Loss 77.25805094838142, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1809, Loss 77.26659712195396, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1810, Loss 77.26301947236061, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1811, Loss 77.26314437389374, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1812, Loss 77.26561415195465, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1813, Loss 77.26144450902939, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1814, Loss 77.25953948497772, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1815, Loss 77.26445570588112, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1816, Loss 77.25988927483559, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1817, Loss 77.26406854391098, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1818, Loss 77.26031374931335, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1819, Loss 77.26332965493202, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1820, Loss 77.26119774580002, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1821, Loss 77.26747840642929, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1822, Loss 77.26087480783463, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1823, Loss 77.26357144117355, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1824, Loss 77.25856792926788, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1825, Loss 77.2585937678814, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1826, Loss 77.25864389538765, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1827, Loss 77.25543156266212, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1828, Loss 77.2604088485241, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1829, Loss 77.26313641667366, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1830, Loss 77.25995817780495, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1831, Loss 77.26475673913956, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1832, Loss 77.26358699798584, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1833, Loss 77.2508093714714, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1834, Loss 77.26492929458618, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1835, Loss 77.25626388192177, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1836, Loss 77.26225394010544, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1837, Loss 77.24768161773682, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1838, Loss 77.26609259843826, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1839, Loss 77.26007631421089, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1840, Loss 77.26268380880356, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1841, Loss 77.26038491725922, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1842, Loss 77.26318818330765, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1843, Loss 77.25918909907341, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1844, Loss 77.26188257336617, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1845, Loss 77.26727810502052, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1846, Loss 77.25946572422981, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1847, Loss 77.2603554725647, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1848, Loss 77.25621238350868, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1849, Loss 77.26645269989967, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1850, Loss 77.25829109549522, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1851, Loss 77.26165664196014, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1852, Loss 77.26209384202957, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1853, Loss 77.26469945907593, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1854, Loss 77.26057609915733, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1855, Loss 77.26183021068573, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1856, Loss 77.26053401827812, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1857, Loss 77.24467965960503, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1858, Loss 77.26339787244797, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1859, Loss 77.25699025392532, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1860, Loss 77.25547257065773, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1861, Loss 77.26315760612488, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1862, Loss 77.25334763526917, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1863, Loss 77.26293471455574, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1864, Loss 77.25592243671417, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1865, Loss 77.25998511910439, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1866, Loss 77.26927798986435, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1867, Loss 77.25243335962296, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1868, Loss 77.26322647929192, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1869, Loss 77.25726883113384, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1870, Loss 77.25808000564575, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1871, Loss 77.26283144950867, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1872, Loss 77.26357761025429, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1873, Loss 77.26124465465546, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1874, Loss 77.26298239827156, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1875, Loss 77.26666906476021, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1876, Loss 77.26563292741776, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1877, Loss 77.25336927175522, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1878, Loss 77.26741841435432, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1879, Loss 77.26330095529556, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1880, Loss 77.26643499732018, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1881, Loss 77.2629945576191, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1882, Loss 77.26379126310349, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1883, Loss 77.25993666052818, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1884, Loss 77.26007238030434, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1885, Loss 77.2650583088398, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1886, Loss 77.26049575209618, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1887, Loss 77.26493608951569, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1888, Loss 77.26027518510818, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1889, Loss 77.26508376002312, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1890, Loss 77.25755152106285, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1891, Loss 77.25952678918839, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1892, Loss 77.25788268446922, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1893, Loss 77.26426813006401, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1894, Loss 77.26212990283966, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1895, Loss 77.26291167736053, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1896, Loss 77.26180338859558, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1897, Loss 77.26042032241821, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1898, Loss 77.26598405838013, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1899, Loss 77.2647143304348, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1900, Loss 77.26548394560814, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1901, Loss 77.24965545535088, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1902, Loss 77.25966975092888, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1903, Loss 77.25913819670677, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1904, Loss 77.26210927963257, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1905, Loss 77.26209333539009, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1906, Loss 77.2580571770668, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1907, Loss 77.26569095253944, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1908, Loss 77.26118630170822, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1909, Loss 77.26117566227913, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1910, Loss 77.2601370215416, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1911, Loss 77.25226813554764, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1912, Loss 77.26017633080482, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1913, Loss 77.25933122634888, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1914, Loss 77.26155391335487, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1915, Loss 77.26142892241478, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1916, Loss 77.25059273838997, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1917, Loss 77.26215133070946, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1918, Loss 77.2637928724289, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1919, Loss 77.25997453927994, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1920, Loss 77.26585245132446, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1921, Loss 77.25861969590187, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1922, Loss 77.26619070768356, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1923, Loss 77.25394102931023, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1924, Loss 77.26770389080048, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1925, Loss 77.26574364304543, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1926, Loss 77.25264778733253, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1927, Loss 77.26413643360138, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1928, Loss 77.25682598352432, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1929, Loss 77.2612901031971, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1930, Loss 77.26413932442665, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1931, Loss 77.24401623010635, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1932, Loss 77.26519018411636, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1933, Loss 77.2539798617363, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1934, Loss 77.26302933692932, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1935, Loss 77.25973153114319, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1936, Loss 77.26433643698692, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1937, Loss 77.26142638921738, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1938, Loss 77.2574332356453, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1939, Loss 77.24992284178734, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1940, Loss 77.25705760717392, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1941, Loss 77.25771337747574, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1942, Loss 77.25937804579735, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1943, Loss 77.26213657855988, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1944, Loss 77.26460790634155, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1945, Loss 77.25364848971367, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1946, Loss 77.25921669602394, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1947, Loss 77.26110717654228, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1948, Loss 77.25445345044136, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1949, Loss 77.25360351800919, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1950, Loss 77.26121431589127, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1951, Loss 77.25721544027328, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1952, Loss 77.26441106200218, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1953, Loss 77.25992691516876, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1954, Loss 77.25886529684067, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1955, Loss 77.25184571743011, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1956, Loss 77.261489123106, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1957, Loss 77.2624374628067, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1958, Loss 77.25483766198158, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1959, Loss 77.25888535380363, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1960, Loss 77.24843826889992, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1961, Loss 77.26479950547218, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1962, Loss 77.26517060399055, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1963, Loss 77.25688469409943, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1964, Loss 77.2606380879879, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1965, Loss 77.26218217611313, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1966, Loss 77.25962379574776, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1967, Loss 77.26223257184029, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1968, Loss 77.26096072793007, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1969, Loss 77.26084300875664, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1970, Loss 77.25844910740852, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1971, Loss 77.26019376516342, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1972, Loss 77.25261870026588, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1973, Loss 77.25907352566719, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1974, Loss 77.26303726434708, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1975, Loss 77.26504009962082, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1976, Loss 77.25058880448341, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1977, Loss 77.26426282525063, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1978, Loss 77.25689479708672, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1979, Loss 77.25719332695007, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1980, Loss 77.2510389983654, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1981, Loss 77.2634542286396, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1982, Loss 77.25600078701973, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1983, Loss 77.26186805963516, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1984, Loss 77.2541534602642, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1985, Loss 77.25741177797318, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n",
      "Epoch 1986, Loss 77.26732856035233, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1987, Loss 77.2583734691143, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1988, Loss 77.2613964676857, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1989, Loss 77.26155969500542, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1990, Loss 77.2603871524334, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1991, Loss 77.2598184645176, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1992, Loss 77.26256373524666, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1993, Loss 77.25723785161972, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1994, Loss 77.25481620430946, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1995, Loss 77.25887900590897, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1996, Loss 77.2584457397461, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1997, Loss 77.26348423957825, Train acc 0.7506421232876712, Test acc 0.7482876712328768\n",
      "Epoch 1998, Loss 77.25748065114021, Train acc 0.7504280821917808, Test acc 0.7482876712328768\n",
      "Epoch 1999, Loss 77.2501682639122, Train acc 0.7508561643835616, Test acc 0.7482876712328768\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "train_acc_history = []\n",
    "test_acc_history = []\n",
    "\n",
    "for epoch in range(2000):\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    loss = train()\n",
    "    loss_history.append(loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "    test_acc_history.append(test_acc)\n",
    "    print(f\"Epoch {epoch}, Loss {loss}, Train acc {train_acc}, Test acc {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc_history, label='train')\n",
    "plt.plot(test_acc_history, label='test')\n",
    "plt.title('Accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WX_FGbp4tV0Z"
   },
   "source": [
    "# Ex13 (optional)\n",
    "\n",
    "\n",
    "\n",
    "1. If we use torch.nn.BCEWithLogitsLoss(), what does we need to change to the definition of the model ?\n",
    "\n",
    "\n",
    "\n",
    "2. The same question for torch.nn.CrossEntropyLoss() loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l-GFUSOmOIr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
